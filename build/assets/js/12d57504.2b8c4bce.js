"use strict";(globalThis.webpackChunkeducational_ai_humanoid_robotics=globalThis.webpackChunkeducational_ai_humanoid_robotics||[]).push([[847],{4838:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"simulation/isaac-sim","title":"Isaac Sim for Advanced Humanoid Robotics","description":"Overview","source":"@site/docs/simulation/isaac-sim.md","sourceDirName":"simulation","slug":"/simulation/isaac-sim","permalink":"/educational-ai-humanoid-robotics/docs/simulation/isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/educational-ai-humanoid-robotics/educational-ai-humanoid-robotics.github.io/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/isaac-sim.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Simulation Environments for Humanoid Robotics","permalink":"/educational-ai-humanoid-robotics/docs/simulation/gazebo"},"next":{"title":"Unity Robotics Simulation for Humanoid Applications","permalink":"/educational-ai-humanoid-robotics/docs/simulation/unity-robotics"}}');var t=i(4848),a=i(8453);const r={sidebar_position:4},s="Isaac Sim for Advanced Humanoid Robotics",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Architecture and Core Components",id:"architecture-and-core-components",level:2},{value:"USD-Based Scene Representation",id:"usd-based-scene-representation",level:3},{value:"Physics Simulation with PhysX",id:"physics-simulation-with-physx",level:3},{value:"Perception Simulation for Humanoid Robots",id:"perception-simulation-for-humanoid-robots",level:2},{value:"Photorealistic Sensor Simulation",id:"photorealistic-sensor-simulation",level:3},{value:"Synthetic Data Generation for Training",id:"synthetic-data-generation-for-training",level:3},{value:"AI Integration and Reinforcement Learning",id:"ai-integration-and-reinforcement-learning",level:2},{value:"Reinforcement Learning Environment",id:"reinforcement-learning-environment",level:3},{value:"Domain Randomization for Sim-to-Real Transfer",id:"domain-randomization-for-sim-to-real-transfer",level:2},{value:"Advanced Domain Randomization Techniques",id:"advanced-domain-randomization-techniques",level:3},{value:"Isaac Sim Extensions for Humanoid Robotics",id:"isaac-sim-extensions-for-humanoid-robotics",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"isaac-sim-for-advanced-humanoid-robotics",children:"Isaac Sim for Advanced Humanoid Robotics"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim by NVIDIA represents the state-of-the-art in high-fidelity simulation for robotics, offering photorealistic rendering, advanced physics simulation, and tight integration with NVIDIA's AI and robotics frameworks. For humanoid robotics, Isaac Sim provides unique capabilities for perception training, physics-based simulation, and AI development."}),"\n",(0,t.jsx)(e.h2,{id:"architecture-and-core-components",children:"Architecture and Core Components"}),"\n",(0,t.jsx)(e.h3,{id:"usd-based-scene-representation",children:"USD-Based Scene Representation"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim uses Universal Scene Description (USD) as its core scene representation format, providing several advantages for humanoid robotics:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Example Isaac Sim setup with USD-based scene construction\nimport omni\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage, get_stage_units\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom pxr import Usd, UsdGeom, Gf, Sdf\n\n# Configure simulation parameters\nconfig = {\n    "headless": False,\n    "window_width": 1280,\n    "window_height": 720,\n    "num_threads": 4,\n    "clear_usd_path_cache": True\n}\n\n# Start Isaac Sim\nsimulation_app = SimulationApp(config)\n\n# Import core Isaac Sim components\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera, LidarRtx\nfrom omni.isaac.motion_generation import ArticulationKinematicTrajectoryGenerator\nfrom omni.isaac.surface_net.core.surface_net import SurfaceNet\n\n# Create world instance with proper units\nworld = World(stage_units_in_meters=1.0)\n\ndef setup_humanoid_simulation_environment():\n    """Setup complete humanoid robot simulation environment"""\n    \n    # Get assets root path from NVIDIA Omniverse\n    assets_root_path = get_assets_root_path()\n    if assets_root_path is None:\n        print("Could not use Isaac Sim assets, using local files instead")\n        asset_path = "/path/to/local/humanoid/robot.usd"\n    else:\n        # Use NVIDIA\'s humanoid robot asset\n        asset_path = assets_root_path + "/Isaac/Robots/Humanoid/humanoid.usd"\n    \n    # Add humanoid robot to the stage\n    add_reference_to_stage(\n        usd_path=asset_path,\n        prim_path="/World/HumanoidRobot"\n    )\n    \n    # Add a simple indoor environment\n    environment_path = assets_root_path + "/Isaac/Environments/Simple_Warehouse/warehouse.usd"\n    add_reference_to_stage(\n        usd_path=environment_path,\n        prim_path="/World/SimpleWarehouse"\n    )\n    \n    # Set up camera for visualization\n    set_camera_view(eye=[5, 5, 5], target=[0, 0, 1])\n    \n    # Define the robot in Isaac Sim\n    robot = Robot(\n        prim_path="/World/HumanoidRobot",\n        name="humanoid_robot",\n        position=[0, 0, 1.0],  # Start 1m above ground\n        orientation=[1, 0, 0, 0]  # Default orientation\n    )\n    \n    return robot\n\n# Initialize the simulation environment\nhumanoid_robot = setup_humanoid_simulation_environment()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"physics-simulation-with-physx",children:"Physics Simulation with PhysX"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim leverages NVIDIA's PhysX engine for accurate physics simulation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from omni.isaac.core.physics_context import PhysicsContext\nfrom omni.isaac.core.prims import RigidPrimView, ArticulationView\nimport numpy as np\n\ndef configure_advanced_physics():\n    """Configure PhysX physics for humanoid robot simulation"""\n    physics_ctx = PhysicsContext(\n        stage=world.stage,\n        # Gravity for Earth-like environment\n        gravity=9.81,\n        # Physics parameters optimized for humanoid robots\n        dt=1.0/60.0,        # 60 Hz physics update rate\n        substeps=8,         # Substeps for stability with fast dynamics\n        solver_type="TGS",  # GPU-based solver for performance\n        use_gpu=True        # Use GPU for physics computation\n    )\n    \n    # Configure PhysX-specific parameters\n    physics_ctx.set_gpu_max_particle_count(1000000)\n    physics_ctx.set_gpu_max_soft_body_contacts(1000000)\n    physics_ctx.set_gpu_max_deformable_contacts(1000000)\n    \n    return physics_ctx\n\ndef setup_robot_physics_properties(robot_prim_path):\n    """Configure physics properties for humanoid robot joints and links"""\n    \n    # Get the robot articulation view\n    robot_view = ArticulationView(\n        prim_path=robot_prim_path,\n        name="humanoid_robot_view",\n        reset_xform_properties=False,\n    )\n    world.add_articulation_view(robot_view, name="humanoid_robot_view")\n    \n    # Configure individual joint properties for realistic humanoid behavior\n    joint_names = [\n        "left_hip_yaw", "left_hip_pitch", "left_hip_roll",\n        "left_knee", "left_ankle_pitch", "left_ankle_roll",\n        "right_hip_yaw", "right_hip_pitch", "right_hip_roll",\n        "right_knee", "right_ankle_pitch", "right_ankle_roll",\n        "left_shoulder_yaw", "left_shoulder_pitch", "left_shoulder_roll",\n        "left_elbow", "right_shoulder_yaw", "right_shoulder_pitch",\n        "right_shoulder_roll", "right_elbow"\n    ]\n    \n    # Configure joint limits and drive parameters\n    joint_positions = np.array([-0.2, -0.4, 0.0, 0.8, -0.4, 0.0] +  # Left leg\n                              [-0.2, -0.4, 0.0, 0.8, -0.4, 0.0] +  # Right leg\n                              [0.0, -0.2, 0.0, 0.5, 0.0, -0.2, 0.0, 0.5])  # Arms\n    \n    # Apply initial configuration\n    robot_view.initialize(world.physics_sim_view)\n    world.reset()\n    \n    # Set initial joint positions\n    robot_view.set_joint_positions(positions=joint_positions)\n    \n    return robot_view\n'})}),"\n",(0,t.jsx)(e.h2,{id:"perception-simulation-for-humanoid-robots",children:"Perception Simulation for Humanoid Robots"}),"\n",(0,t.jsx)(e.h3,{id:"photorealistic-sensor-simulation",children:"Photorealistic Sensor Simulation"}),"\n",(0,t.jsx)(e.p,{children:"One of Isaac Sim's key strengths is its ability to simulate realistic sensors with photorealistic rendering:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from omni.isaac.sensor import Camera, LidarRtx\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nimport carb\nimport numpy as np\n\nclass HumanoidPerceptionSystem:\n    def __init__(self, robot_prim_path):\n        self.robot_prim_path = robot_prim_path\n        self.cameras = []\n        self.lidars = []\n        self.imus = []\n        \n        self.setup_perception_sensors()\n    \n    def setup_perception_sensors(self):\n        """Setup various perception sensors for the humanoid robot"""\n        \n        # Head-mounted RGB camera for vision\n        head_camera = Camera(\n            prim_path=self.robot_prim_path + "/Head/head_camera",\n            frequency=30,  # 30 Hz\n            resolution=(640, 480),\n            position=carb.Float3(0.0, 0.0, 0.15),  # Position 15cm forward on head\n            orientation=carb.QuatF(0, 0, 0, 1)\n        )\n        # Attach to robot\n        head_camera.add_modifications_to_prim(get_prim_at_path(self.robot_prim_path + "/Head"))\n        self.cameras.append(head_camera)\n        \n        # Chest-mounted depth camera\n        depth_camera = Camera(\n            prim_path=self.robot_prim_path + "/Base/chest_depth_camera",\n            frequency=30,\n            resolution=(320, 240),\n            position=carb.Float3(0.0, 0.0, 0.8),  # Position on chest\n        )\n        self.cameras.append(depth_camera)\n        \n        # LiDAR for 3D perception\n        lidar = LidarRtx(\n            prim_path=self.robot_prim_path + "/Base/lidar",\n            translation=carb.Float3(0.0, 0.0, 0.9),  # Position on upper body\n            config="Example_Rotary",\n            orientation=carb.QuatF(0, 0, 0, 1)\n        )\n        self.lidars.append(lidar)\n        \n        print(f"Setup {len(self.cameras)} cameras and {len(self.lidars)} lidars")\n    \n    def get_camera_data(self, camera_index=0):\n        """Get data from specified camera"""\n        if camera_index < len(self.cameras):\n            camera = self.cameras[camera_index]\n            rgb_data = camera.get_rgb()\n            depth_data = camera.get_depth()\n            segmentation_data = camera.get_semantic_segmentation()\n            \n            return {\n                "rgb": rgb_data,\n                "depth": depth_data,\n                "segmentation": segmentation_data,\n                "timestamp": camera.get_current_frame()\n            }\n        return None\n    \n    def get_lidar_data(self, lidar_index=0):\n        """Get data from specified LiDAR"""\n        if lidar_index < len(self.lidars):\n            lidar = self.lidars[lidar_index]\n            return lidar.get_point_cloud()\n        return None\n    \n    def simulate_vlm_perception(self, instruction, camera_data):\n        """Simulate Vision-Language Model perception for humanoid robot"""\n        \n        # In a real implementation, this would connect to actual VLM models\n        # For simulation, we\'ll mock the perception results\n        perception_results = {\n            "objects_detected": ["person", "table", "chair", "cup"],\n            "object_positions": {\n                "person": [2.0, 1.0, 0.0],\n                "table": [1.5, 0.0, 0.0],\n                "chair": [1.5, -0.5, 0.0],\n                "cup": [1.55, 0.05, 0.75]\n            },\n            "spatial_relationships": [\n                "person is sitting at table",\n                "cup is on table"\n            ],\n            "action_affordances": [\n                "approach person for greeting",\n                "pick up cup",\n                "navigate to table"\n            ],\n            "instruction_understanding": f"Instruction \'{instruction}\' understood",\n            "confidence": 0.85\n        }\n        \n        return perception_results\n\n# Example usage\nperception_system = HumanoidPerceptionSystem("/World/HumanoidRobot")\n'})}),"\n",(0,t.jsx)(e.h3,{id:"synthetic-data-generation-for-training",children:"Synthetic Data Generation for Training"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim excels at generating synthetic training data for AI models:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.synthetic_utils.custom_domain_randomizer import CustomDomainRandomizer\nimport random\nimport os\n\nclass SyntheticDataGenerator:\n    def __init__(self, robot_env, output_dir="synthetic_data"):\n        self.robot_env = robot_env\n        self.output_dir = output_dir\n        self.domain_randomizer = CustomDomainRandomizer()\n        \n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n        os.makedirs(os.path.join(output_dir, "images"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, "labels"), exist_ok=True)\n        \n    def randomize_environment(self):\n        """Apply domain randomization to the environment"""\n        \n        # Randomize lighting conditions\n        lights = self.robot_env.get_lights()\n        for light in lights:\n            new_intensity = random.uniform(100, 1000)\n            new_color = [random.uniform(0.8, 1.2) for _ in range(3)]\n            \n            # Apply randomization to light parameters\n            # (Implementation would use USD API to modify light properties)\n        \n        # Randomize material properties\n        objects = self.robot_env.get_movable_objects()\n        for obj in objects:\n            # Randomize texture, color, reflectance, etc.\n            new_color = [random.uniform(0.0, 1.0) for _ in range(3)]\n            new_roughness = random.uniform(0.1, 0.9)\n            new_metallic = random.uniform(0.0, 0.1)\n            \n            # Apply material changes\n            # (Implementation would modify USD material properties)\n    \n    def generate_training_data(self, num_samples=1000, task_descriptions=None):\n        """Generate synthetic training data for humanoid robot tasks"""\n        \n        if task_descriptions is None:\n            task_descriptions = [\n                "person waving", "person sitting", "person walking",\n                "object on table", "object on floor", "object in hand"\n            ]\n        \n        data_samples = []\n        \n        for i in range(num_samples):\n            # Randomize environment\n            self.randomize_environment()\n            \n            # Select a random task\n            task_description = random.choice(task_descriptions)\n            \n            # Set up the scene according to the task\n            self.setup_task_scene(task_description)\n            \n            # Capture sensor data\n            camera_data = self.robot_env.get_camera_data()\n            lidar_data = self.robot_env.get_lidar_data()\n            \n            # Generate labels (ground truth)\n            labels = self.generate_labels(task_description, camera_data)\n            \n            # Save data\n            sample_data = {\n                "image_path": f"{self.output_dir}/images/sample_{i:05d}.png",\n                "lidar_path": f"{self.output_dir}/data/lidar_{i:05d}.npy",\n                "labels_path": f"{self.output_dir}/labels/labels_{i:05d}.json",\n                "task": task_description,\n                "timestamp": i\n            }\n            \n            # Save image and labels (implementation would save to files)\n            # Save RGB image\n            # Save labels in COCO or similar format\n            # Save additional sensor data\n            \n            data_samples.append(sample_data)\n            \n            if i % 100 == 0:\n                print(f"Generated {i} samples out of {num_samples}")\n        \n        return data_samples\n    \n    def setup_task_scene(self, task_description):\n        """Setup the scene according to a task description"""\n        \n        # This would position objects, humans, and robot according to the task\n        # For example, for "person waving", position a human model with an appropriate pose\n        pass\n    \n    def generate_labels(self, task_description, camera_data):\n        """Generate ground truth labels for the training data"""\n        \n        # Based on the task and scene setup, generate appropriate labels\n        # This could include bounding boxes, segmentation masks, keypoint labels, etc.\n        labels = {\n            "task": task_description,\n            "objects_present": [],\n            "bounding_boxes": [],\n            "segmentation_masks": [],\n            "keypoints": [],\n            "spatial_relationships": []\n        }\n        \n        return labels\n'})}),"\n",(0,t.jsx)(e.h2,{id:"ai-integration-and-reinforcement-learning",children:"AI Integration and Reinforcement Learning"}),"\n",(0,t.jsx)(e.h3,{id:"reinforcement-learning-environment",children:"Reinforcement Learning Environment"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim provides excellent support for reinforcement learning applications in humanoid robotics:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\nimport numpy as np\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.articulations import ArticulationView\nfrom omni.isaac.core.utils.torch.maths import torch_acos, torch_normalize, torch_cross, torch_dot\nfrom omni.isaac.core.utils.torch.rotations import *\nfrom pxr import Gf, Usd, UsdGeom\n\nclass HumanoidRLEnvironment:\n    """Reinforcement Learning environment for humanoid robot in Isaac Sim"""\n    \n    def __init__(self, robot_prim_path="/World/HumanoidRobot", num_envs=1):\n        self.robot_prim_path = robot_prim_path\n        self.num_envs = num_envs\n        \n        # RL-specific parameters\n        self.max_episode_length = 1000  # 1000 time steps\n        self.action_scale = 0.5  # Scale factor for actions\n        self.velocity_scale = 0.2  # Scale factor for velocities\n        self.angular_velocity_scale = 0.5  # Scale factor for angular velocities\n        \n        # Define action and observation spaces\n        self.action_space_size = 20  # Example: 20 DOF humanoid\n        self.observation_space_size = 48  # State + history size\n        \n        # Initialize robot view for batch operations\n        self.robot_view = ArticulationView(\n            prim_paths_expr="/World/envs/.*/HumanoidRobot",\n            name="humanoid_robot_view",\n            reset_xform_properties=False,\n        )\n        world.add_articulation_view(self.robot_view, name="humanoid_robot_view")\n        \n        # Track episode steps\n        self.current_episode_steps = torch.zeros(self.num_envs, dtype=torch.int32, device="cpu")\n        \n    def reset(self, env_ids=None):\n        """Reset the environment for specified environments"""\n        if env_ids is None:\n            env_ids = torch.arange(self.num_envs, dtype=torch.int32, device="cpu")\n        \n        # Reset joint positions to nominal standing pose\n        reset_joint_positions = torch.zeros(\n            len(env_ids), self.action_space_size, device="cpu"\n        )\n        \n        # Standing pose: slight bend in knees, arms at sides\n        reset_joint_positions[:, 3] = 0.8   # Left knee\n        reset_joint_positions[:, 9] = 0.8   # Right knee\n        reset_joint_positions[:, 13] = -0.2 # Left shoulder pitch\n        reset_joint_positions[:, 17] = -0.2 # Right shoulder pitch\n        \n        # Reset joint velocities to zero\n        reset_joint_velocities = torch.zeros(\n            len(env_ids), self.action_space_size, device="cpu"\n        )\n        \n        # Apply resets\n        self.robot_view.set_joint_positions(reset_joint_positions, indices=env_ids)\n        self.robot_view.set_joint_velocities(reset_joint_velocities, indices=env_ids)\n        \n        # Reset episode steps\n        self.current_episode_steps[env_ids] = 0\n        \n        # Compute initial observations\n        observations = self.compute_observations(env_ids)\n        \n        # Reset any additional environment state\n        self.reset_additional_state(env_ids)\n        \n        return observations\n    \n    def compute_observations(self, env_ids=None):\n        """Compute observations for the humanoid robot"""\n        if env_ids is None:\n            env_ids = torch.arange(self.num_envs, dtype=torch.int32, device="cpu")\n        \n        # Get current states from the robot\n        current_joint_positions = self.robot_view.get_joint_positions(clone=True)\n        current_joint_velocities = self.robot_view.get_joint_velocities(clone=True)\n        \n        # Get root pose and velocity\n        root_states = self.robot_view.get_world_poses(clone=True)\n        root_positions = root_states[0]\n        root_orientations = root_states[1]\n        \n        root_lin_vel = self.robot_view.get_linear_velocities(clone=True)\n        root_ang_vel = self.robot_view.get_angular_velocities(clone=True)\n        \n        # Calculate observations\n        obs = torch.cat([\n            # Joint positions (scaled)\n            torch.clamp(current_joint_positions * 2.0, -5.0, 5.0),\n            \n            # Joint velocities (scaled) \n            current_joint_velocities * self.velocity_scale,\n            \n            # Root positions (relative to world)\n            root_positions[:, 2:3] - 0.8,  # Height above ground (assuming 0.8m hip height)\n            \n            # Root orientations\n            quat_to_angle_axis(root_orientations)[:, 0:3],  # Only take the axis-angle representation\n            \n            # Root linear and angular velocities\n            root_lin_vel * self.velocity_scale,\n            root_ang_vel * self.angular_velocity_scale,\n            \n            # Commands (for goal-based tasks, placeholder)\n            torch.zeros((len(env_ids), 4), device="cpu", dtype=torch.float32)\n        ], dim=-1)\n        \n        return obs\n    \n    def compute_rewards(self, actions):\n        """Compute rewards for the humanoid robot"""\n        \n        # Get current states\n        current_joint_positions = self.robot_view.get_joint_positions(clone=True)\n        current_joint_velocities = self.robot_view.get_joint_velocities(clone=True)\n        \n        root_positions, root_orientations = self.robot_view.get_world_poses(clone=True)\n        root_lin_vel = self.robot_view.get_linear_velocities(clone=True)\n        root_ang_vel = self.robot_view.get_angular_velocities(clone=True)\n        \n        # Reward for forward progress\n        target_velocity = 1.0  # m/s\n        forward_velocity = root_lin_vel[:, 0]  # x-component\n        forward_progress_reward = torch.clamp(forward_velocity, 0.0, target_velocity) / target_velocity\n        \n        # Penalty for falling\n        robot_height = root_positions[:, 2]\n        height_threshold = 0.5  # Robot is considered fallen if below this height\n        fall_penalty = torch.where(\n            robot_height < height_threshold, \n            torch.tensor(-1.0, device="cpu"), \n            torch.tensor(0.0, device="cpu")\n        )\n        \n        # Penalty for high action rate (encourage smooth movement)\n        action_rate_penalty = torch.sum(torch.square(actions), dim=-1) * 0.001\n        \n        # Reward for upright posture\n        roll, pitch, yaw = self.get_euler_from_quaternion(root_orientations)\n        upright_reward = torch.exp(-torch.abs(pitch)) * torch.exp(-torch.abs(roll))\n        \n        # Total reward\n        total_reward = (\n            2.0 * forward_progress_reward + \n            3.0 * upright_reward + \n            0.1 * fall_penalty - \n            action_rate_penalty\n        )\n        \n        return total_reward\n    \n    def compute_terminations(self):\n        """Compute termination conditions"""\n        \n        root_positions, _ = self.robot_view.get_world_poses(clone=True)\n        robot_height = root_positions[:, 2]\n        \n        # Terminate if robot falls\n        falls = robot_height < 0.5  # If height below 0.5m (fell)\n        \n        # Terminate if episode is too long\n        time_outs = self.current_episode_steps >= self.max_episode_length\n        \n        # Terminate if robot moves too far from starting position\n        x_pos = torch.abs(root_positions[:, 0])\n        y_pos = torch.abs(root_positions[:, 1])\n        out_of_bounds = (x_pos > 10.0) | (y_pos > 10.0)  # 10m boundary\n        \n        terminations = falls | time_outs | out_of_bounds\n        \n        return terminations\n    \n    def pre_physics_step(self, actions):\n        """Apply actions to the environment before physics step"""\n        \n        # Scale and apply actions\n        scaled_actions = actions * self.action_scale\n        \n        # Add actions to current joint velocities\n        current_joint_velocities = self.robot_view.get_joint_velocities(clone=True)\n        new_joint_velocities = current_joint_velocities + scaled_actions\n        \n        # Apply new velocities with limits\n        joint_vel_limits = self.robot_view.get_joint_velocity_limits(clone=True)\n        new_joint_velocities = torch.clamp(\n            new_joint_velocities,\n            min=-joint_vel_limits,\n            max=joint_vel_limits\n        )\n        \n        self.robot_view.set_joint_velocities(new_joint_velocities)\n        \n        # Increment episode step counters\n        self.current_episode_steps += 1\n    \n    def get_euler_from_quaternion(self, quaternions):\n        """Convert quaternions to Euler angles"""\n        # Simplified conversion for pitch and roll (assumes yaw is not critical)\n        w, x, y, z = quaternions[:, 0], quaternions[:, 1], quaternions[:, 2], quaternions[:, 3]\n        \n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = torch.atan2(sinr_cosp, cosr_cosp)\n        \n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        pitch = torch.where(\n            torch.abs(sinp) >= 1,\n            torch.copysign(torch.tensor(np.pi) / 2, sinp),  # Use 90 degrees if out of range\n            torch.asin(sinp)\n        )\n        \n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = torch.atan2(siny_cosp, cosy_cosp)\n        \n        return roll, pitch, yaw\n\ndef quat_to_angle_axis(quat):\n    """Convert quaternion to angle-axis representation"""\n    # Extract real and imaginary parts\n    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\n    \n    # Calculate angle\n    angle = 2 * torch.acos(torch.clamp(w, min=-1.0, max=1.0))\n    \n    # Calculate normalized axis\n    sin_half_angle = torch.sqrt(1 - w * w + 1e-8)  # Add small number to prevent division by zero\n    axis_x = x / sin_half_angle\n    axis_y = y / sin_half_angle  \n    axis_z = z / sin_half_angle\n    \n    # Return angle-axis representation\n    return torch.stack([axis_x, axis_y, axis_z], dim=-1) * angle.unsqueeze(-1)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"domain-randomization-for-sim-to-real-transfer",children:"Domain Randomization for Sim-to-Real Transfer"}),"\n",(0,t.jsx)(e.h3,{id:"advanced-domain-randomization-techniques",children:"Advanced Domain Randomization Techniques"}),"\n",(0,t.jsx)(e.p,{children:"Domain randomization is crucial for transferring models trained in simulation to real robots:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from omni.isaac.synthetic_utils.custom_domain_randomizer import CustomDomainRandomizer\nfrom pxr import Usd, Sdf, Gf\nimport random\n\nclass AdvancedDomainRandomizer:\n    def __init__(self):\n        self.randomization_params = {\n            "lighting": {\n                "intensity_range": (100, 1000),\n                "color_temperature_range": (3000, 8000),\n                "directional_variance": (0.1, 0.5)\n            },\n            "materials": {\n                "albedo_range": ((0.1, 0.1, 0.1), (1.0, 1.0, 1.0)),\n                "roughness_range": (0.1, 1.0),\n                "metallic_range": (0.0, 0.1),\n                "specular_range": (0.0, 1.0)\n            },\n            "dynamics": {\n                "mass_multiplier_range": (0.8, 1.2),\n                "friction_range": (0.1, 1.5),\n                "restitution_range": (0.0, 0.5)\n            },\n            "sensors": {\n                "noise_std_range": (0.001, 0.01),\n                "bias_range": (-0.01, 0.01)\n            }\n        }\n        \n    def randomize_lighting(self):\n        """Randomize lighting conditions in the scene"""\n        # Get all lights in the scene\n        lights = self.get_lights_in_scene()\n        \n        for light_prim in lights:\n            # Randomize intensity\n            intensity = random.uniform(\n                self.randomization_params["lighting"]["intensity_range"][0],\n                self.randomization_params["lighting"]["intensity_range"][1]\n            )\n            # Apply to light via USD API (implementation specific)\n            \n            # Randomize color temperature\n            color_temp = random.uniform(\n                self.randomization_params["lighting"]["color_temperature_range"][0],\n                self.randomization_params["lighting"]["color_temperature_range"][1]\n            )\n            # Convert to RGB and apply\n            \n            # Randomize direction slightly\n            variance = self.randomization_params["lighting"]["directional_variance"]\n            # Apply small random rotations to light direction\n    \n    def randomize_materials(self):\n        """Randomize material properties of objects"""\n        objects = self.get_all_objects_in_scene()\n        \n        for obj_prim in objects:\n            # Randomize appearance properties\n            albedo_min, albedo_max = self.randomization_params["materials"]["albedo_range"]\n            albedo = [\n                random.uniform(albedo_min[i], albedo_max[i]) for i in range(3)\n            ]\n            \n            roughness = random.uniform(\n                self.randomization_params["materials"]["roughness_range"][0],\n                self.randomization_params["materials"]["roughness_range"][1]\n            )\n            \n            metallic = random.uniform(\n                self.randomization_params["materials"]["metallic_range"][0],\n                self.randomization_params["materials"]["metallic_range"][1]\n            )\n            \n            # Apply material properties to object\n            # This would involve creating or modifying USD material definitions\n    \n    def randomize_dynamics(self):\n        """Randomize dynamic properties for sim-to-real transfer"""\n        \n        # Get all rigid bodies in the scene\n        bodies = self.get_all_rigid_bodies()\n        \n        for body_prim in bodies:\n            # Randomize mass\n            mass_multiplier = random.uniform(\n                self.randomization_params["dynamics"]["mass_multiplier_range"][0],\n                self.randomization_params["dynamics"]["mass_multiplier_range"][1]\n            )\n            \n            # Randomize friction\n            friction = random.uniform(\n                self.randomization_params["dynamics"]["friction_range"][0],\n                self.randomization_params["dynamics"]["friction_range"][1]\n            )\n            \n            # Randomize restitution\n            restitution = random.uniform(\n                self.randomization_params["dynamics"]["restitution_range"][0],\n                self.randomization_params["dynamics"]["restitution_range"][1]\n            )\n            \n            # Apply dynamics parameters\n            # This would modify PhysX properties via USD schema\n'})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-sim-extensions-for-humanoid-robotics",children:"Isaac Sim Extensions for Humanoid Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Creating custom extensions can enhance Isaac Sim for specific humanoid robotics applications:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import omni.ext\nimport omni.kit.ui\nfrom omni.kit.menu import Menu\n\nclass HumanoidRobotExtension(omni.ext.IExt):\n    """Extension for humanoid robotics in Isaac Sim"""\n    \n    def on_startup(self, ext_id):\n        self._ext_id = ext_id\n        \n        # Create menu items for humanoid robotics tools\n        self._menu = Menu()\n        \n        # Add tools to Isaac Sim UI\n        self._menu.add_item("Humanoid Robotics/Setup Environment", self.setup_humanoid_environment)\n        self._menu.add_item("Humanoid Robotics/Generate Training Data", self.generate_training_data)\n        self._menu.add_item("Humanoid Robotics/Evaluate Policies", self.evaluate_policies)\n        \n        print("[humanoid_robotics] Humanoid Robotics extension loaded")\n    \n    def on_shutdown(self):\n        print("[humanoid_robotics] Humanoid Robotics extension shutdown")\n        \n        # Clean up menu items\n        if self._menu:\n            self._menu.destroy()\n            self._menu = None\n    \n    def setup_humanoid_environment(self):\n        """Setup a humanoid robot environment"""\n        # Implementation would create a standard humanoid robot scene\n        print("Setting up humanoid robot environment...")\n        # Add default humanoid robot, environment, and sensors\n    \n    def generate_training_data(self):\n        """Generate synthetic training data using domain randomization"""\n        print("Generating synthetic training data...")\n        # Implementation of synthetic data generation pipeline\n    \n    def evaluate_policies(self):\n        """Evaluate robot control policies"""\n        print("Evaluating robot control policies...")\n        # Implementation of policy evaluation tools\n'})}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Using the educational AI agents, create an Isaac Sim environment for training a humanoid robot to walk using reinforcement learning."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Implement domain randomization to improve the sim-to-real transfer performance of your walking controller."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Consider how you would validate that your Isaac Sim environment accurately represents the real robot's dynamics and perception capabilities."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The next section will explore Unity Robotics Simulation and how it differs from Isaac Sim and Gazebo for humanoid robotics applications."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var o=i(6540);const t={},a=o.createContext(t);function r(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);