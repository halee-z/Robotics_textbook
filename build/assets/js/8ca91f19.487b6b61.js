"use strict";(globalThis.webpackChunkeducational_ai_humanoid_robotics=globalThis.webpackChunkeducational_ai_humanoid_robotics||[]).push([[253],{8453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>o});var s=i(6540);const t={},a=s.createContext(t);function d(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),s.createElement(a.Provider,{value:n},e.children)}},9433:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>m,frontMatter:()=>d,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"vlm/embedding-techniques","title":"Embedding Techniques for Robotics Applications","description":"Overview","source":"@site/docs/vlm/embedding-techniques.md","sourceDirName":"vlm","slug":"/vlm/embedding-techniques","permalink":"/educational-ai-humanoid-robotics/docs/vlm/embedding-techniques","draft":false,"unlisted":false,"editUrl":"https://github.com/educational-ai-humanoid-robotics/educational-ai-humanoid-robotics.github.io/tree/main/packages/create-docusaurus/templates/shared/docs/vlm/embedding-techniques.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language Model Architectures for Robotics","permalink":"/educational-ai-humanoid-robotics/docs/vlm/vla-architectures"},"next":{"title":"Planning with Vision-Language Models in Robotics","permalink":"/educational-ai-humanoid-robotics/docs/vlm/planning-with-vlm"}}');var t=i(4848),a=i(8453);const d={sidebar_position:3},o="Embedding Techniques for Robotics Applications",r={},l=[{value:"Overview",id:"overview",level:2},{value:"Visual Embeddings",id:"visual-embeddings",level:2},{value:"Feature Extraction Methods",id:"feature-extraction-methods",level:3},{value:"Convolutional Neural Networks (CNNs)",id:"convolutional-neural-networks-cnns",level:4},{value:"Vision Transformers (ViTs)",id:"vision-transformers-vits",level:4},{value:"Multi-Scale Embeddings",id:"multi-scale-embeddings",level:3},{value:"Language Embeddings",id:"language-embeddings",level:2},{value:"Pre-trained Language Models",id:"pre-trained-language-models",level:3},{value:"BERT-based Embeddings",id:"bert-based-embeddings",level:4},{value:"Sentence Transformers",id:"sentence-transformers",level:4},{value:"Cross-Modal Embeddings",id:"cross-modal-embeddings",level:2},{value:"CLIP Embeddings",id:"clip-embeddings",level:3},{value:"Robot-Specific Embeddings",id:"robot-specific-embeddings",level:3},{value:"Embedding Alignment Techniques",id:"embedding-alignment-techniques",level:2},{value:"Contrastive Learning",id:"contrastive-learning",level:3},{value:"Triplet Embeddings",id:"triplet-embeddings",level:3},{value:"Embedding Optimization for Robotics",id:"embedding-optimization-for-robotics",level:2},{value:"Memory-Efficient Embeddings",id:"memory-efficient-embeddings",level:3},{value:"Continual Embedding Updates",id:"continual-embedding-updates",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"embedding-techniques-for-robotics-applications",children:"Embedding Techniques for Robotics Applications"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Embedding techniques are fundamental to Vision-Language Models in robotics, enabling the conversion of high-dimensional sensory data into meaningful representations that can guide robot behavior. This section covers the key embedding methods that enable robots to understand and interact with their environment."}),"\n",(0,t.jsx)(n.h2,{id:"visual-embeddings",children:"Visual Embeddings"}),"\n",(0,t.jsx)(n.h3,{id:"feature-extraction-methods",children:"Feature Extraction Methods"}),"\n",(0,t.jsx)(n.h4,{id:"convolutional-neural-networks-cnns",children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,t.jsx)(n.p,{children:"Traditional CNNs extract hierarchical visual features from images:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass CNNEncoder(nn.Module):\n    def __init__(self, output_dim=512):\n        super().__init__()\n        # Pre-trained ResNet for feature extraction\n        from torchvision.models import resnet50\n        resnet = resnet50(pretrained=True)\n        \n        # Remove the final classification layer\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection = nn.Linear(2048, output_dim)  # ResNet50 outputs 2048-dim features\n    \n    def forward(self, images):\n        # Extract features\n        features = self.backbone(images)\n        features = torch.flatten(features, 1)\n        \n        # Project to desired dimension\n        embeddings = self.projection(features)\n        \n        return embeddings\n"})}),"\n",(0,t.jsx)(n.h4,{id:"vision-transformers-vits",children:"Vision Transformers (ViTs)"}),"\n",(0,t.jsx)(n.p,{children:"ViTs process images as sequences of patches, often providing better representations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from transformers import ViTModel, ViTConfig\n\nclass ViTEmbedder(nn.Module):\n    def __init__(self, output_dim=512):\n        super().__init__()\n        config = ViTConfig.from_pretrained("google/vit-base-patch16-224")\n        self.vit = ViTModel.from_pretrained("google/vit-base-patch16-224", config=config)\n        self.projection = nn.Linear(self.vit.config.hidden_size, output_dim)\n    \n    def forward(self, pixel_values):\n        outputs = self.vit(pixel_values)\n        # Use the pooled output (cls token)\n        sequence_output = outputs.pooler_output\n        embeddings = self.projection(sequence_output)\n        return embeddings\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multi-scale-embeddings",children:"Multi-Scale Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"Robots often need to process visual information at multiple scales:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class MultiScaleVisualEmbedder(nn.Module):\n    def __init__(self, output_dim=512):\n        super().__init__()\n        # Different scales for different purposes\n        self.fine_grained = ViTEmbedder(output_dim=output_dim//2)  # For object details\n        self.contextual = CNNEncoder(output_dim=output_dim//2)    # For scene context\n    \n    def forward(self, image, region_of_interest=None):\n        # Context embedding from full image\n        context_embedding = self.contextual(image)\n        \n        # Fine-grained embedding from region of interest or full image\n        if region_of_interest is not None:\n            # Crop and process specific region\n            cropped_image = self.crop_image(image, region_of_interest)\n            fine_embedding = self.fine_grained(cropped_image)\n        else:\n            fine_embedding = self.fine_grained(image)\n        \n        # Combine embeddings\n        combined_embedding = torch.cat([context_embedding, fine_embedding], dim=-1)\n        \n        return combined_embedding\n    \n    def crop_image(self, image, bbox):\n        # Implementation for cropping image based on bounding box\n        # bbox format: [x1, y1, x2, y2]\n        pass\n"})}),"\n",(0,t.jsx)(n.h2,{id:"language-embeddings",children:"Language Embeddings"}),"\n",(0,t.jsx)(n.h3,{id:"pre-trained-language-models",children:"Pre-trained Language Models"}),"\n",(0,t.jsx)(n.h4,{id:"bert-based-embeddings",children:"BERT-based Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"BERT provides contextual language understanding:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from transformers import BertModel, BertTokenizer\n\nclass BERTLanguageEmbedder:\n    def __init__(self, model_name="bert-base-uncased"):\n        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n        self.model = BertModel.from_pretrained(model_name)\n    \n    def embed_text(self, text):\n        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            # Use the [CLS] token embedding for the entire sentence\n            embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        \n        return embedding\n'})}),"\n",(0,t.jsx)(n.h4,{id:"sentence-transformers",children:"Sentence Transformers"}),"\n",(0,t.jsx)(n.p,{children:"For robotics applications requiring semantic similarity:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sentence_transformers import SentenceTransformer\n\nclass RoboticLanguageEncoder:\n    def __init__(self, model_name="all-MiniLM-L6-v2"):\n        self.model = SentenceTransformer(model_name)\n    \n    def encode_sentences(self, sentences):\n        """Encode multiple sentences into embeddings"""\n        embeddings = self.model.encode(sentences, convert_to_tensor=True)\n        return embeddings\n    \n    def compute_similarity(self, sentence1, sentence2):\n        """Compute semantic similarity between two sentences"""\n        emb1 = self.encode_sentences([sentence1])\n        emb2 = self.encode_sentences([sentence2])\n        similarity = torch.cosine_similarity(emb1, emb2, dim=1)\n        return similarity.item()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"cross-modal-embeddings",children:"Cross-Modal Embeddings"}),"\n",(0,t.jsx)(n.h3,{id:"clip-embeddings",children:"CLIP Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"CLIP creates a shared embedding space for visual and textual information:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import clip\nimport torch\nfrom PIL import Image\n\nclass CLIPEmbedder:\n    def __init__(self, model_name="ViT-B/32"):\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model, self.preprocess = clip.load(model_name, device=self.device)\n    \n    def embed_image(self, image_path):\n        """Embed an image into the CLIP space"""\n        image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            image_features = self.model.encode_image(image)\n            # Normalize embeddings\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n        \n        return image_features.cpu()\n    \n    def embed_text(self, text):\n        """Embed text into the CLIP space"""\n        text_tokens = clip.tokenize([text]).to(self.device)\n        \n        with torch.no_grad():\n            text_features = self.model.encode_text(text_tokens)\n            # Normalize embeddings\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n        \n        return text_features.cpu()\n    \n    def compute_similarity(self, image_path, text):\n        """Compute similarity between image and text"""\n        image_features = self.embed_image(image_path)\n        text_features = self.embed_text(text)\n        \n        similarity = (image_features @ text_features.T).item()\n        return similarity\n'})}),"\n",(0,t.jsx)(n.h3,{id:"robot-specific-embeddings",children:"Robot-Specific Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"Creating embeddings that are specifically optimized for robotics tasks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class RoboticActionEmbedder(nn.Module):\n    def __init__(self, vocab_size, action_space_dim, embedding_dim=512):\n        super().__init__()\n        \n        # Embed language instructions\n        self.lang_embedder = nn.Embedding(vocab_size, embedding_dim)\n        \n        # Embed visual scene context\n        self.visual_encoder = ViTEmbedder(output_dim=embedding_dim)\n        \n        # Embed current robot state\n        self.state_encoder = nn.Sequential(\n            nn.Linear(6, 128),  # Position and orientation\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, embedding_dim//2)\n        )\n        \n        # Fuse all modalities\n        self.fusion = nn.Sequential(\n            nn.Linear(embedding_dim * 2 + embedding_dim//2, embedding_dim),\n            nn.ReLU(),\n            nn.Linear(embedding_dim, embedding_dim)\n        )\n        \n        # Map to action space\n        self.action_head = nn.Linear(embedding_dim, action_space_dim)\n    \n    def forward(self, instruction_ids, visual_input, robot_state):\n        # Embed language instruction\n        lang_embedding = self.lang_embedder(instruction_ids).mean(dim=1)  # Average over sequence\n        \n        # Embed visual input\n        visual_embedding = self.visual_encoder(visual_input)\n        \n        # Embed robot state\n        state_embedding = self.state_encoder(robot_state)\n        \n        # Concatenate all embeddings\n        fused_input = torch.cat([lang_embedding, visual_embedding, state_embedding], dim=-1)\n        \n        # Fuse and generate action embedding\n        fused_embedding = self.fusion(fused_input)\n        \n        # Generate action\n        action = self.action_head(fused_embedding)\n        \n        return action, fused_embedding\n"})}),"\n",(0,t.jsx)(n.h2,{id:"embedding-alignment-techniques",children:"Embedding Alignment Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"contrastive-learning",children:"Contrastive Learning"}),"\n",(0,t.jsx)(n.p,{children:"Align visual and language embeddings using contrastive loss:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ContrastiveAligner(nn.Module):\n    def __init__(self, embedding_dim=512, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        \n        # Visual and text encoders\n        self.visual_encoder = ViTEmbedder(output_dim=embedding_dim)\n        self.text_encoder = RoboticLanguageEncoder()  # Simplified\n        \n        # Learnable temperature parameter\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n    \n    def forward(self, images, texts):\n        # Encode images\n        image_features = self.visual_encoder(images)\n        image_features = F.normalize(image_features, dim=1)\n        \n        # Encode texts\n        text_features = self.text_encoder.encode_texts(texts)\n        text_features = F.normalize(text_features, dim=1)\n        \n        # Compute logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logit_scale * text_features @ image_features.t()\n        \n        # Compute contrastive loss\n        batch_size = images.shape[0]\n        labels = torch.arange(batch_size, device=images.device)\n        \n        loss_i = F.cross_entropy(logits_per_image, labels)\n        loss_t = F.cross_entropy(logits_per_text, labels)\n        loss = (loss_i + loss_t) / 2\n        \n        return loss, image_features, text_features\n"})}),"\n",(0,t.jsx)(n.h3,{id:"triplet-embeddings",children:"Triplet Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"Using triplet loss for improved embedding quality:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class TripletEmbedder(nn.Module):\n    def __init__(self, embedding_dim=512, margin=0.2):\n        super().__init__()\n        self.margin = margin\n        self.visual_encoder = ViTEmbedder(output_dim=embedding_dim)\n        self.text_encoder = BERTLanguageEmbedder()\n        \n    def forward(self, anchor_images, positive_texts, negative_texts):\n        # Encode all inputs\n        anchor_embeddings = self.visual_encoder(anchor_images)\n        pos_text_embeddings = self.text_encoder.embed_text(positive_texts)\n        neg_text_embeddings = self.text_encoder.embed_text(negative_texts)\n        \n        # Compute triplet loss\n        pos_distance = F.pairwise_distance(anchor_embeddings, pos_text_embeddings)\n        neg_distance = F.pairwise_distance(anchor_embeddings, neg_text_embeddings)\n        \n        loss = F.relu(pos_distance - neg_distance + self.margin)\n        loss = loss.mean()\n        \n        return loss\n"})}),"\n",(0,t.jsx)(n.h2,{id:"embedding-optimization-for-robotics",children:"Embedding Optimization for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"memory-efficient-embeddings",children:"Memory-Efficient Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"For resource-constrained robotics platforms:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class QuantizedRoboticEmbedder:\n    def __init__(self, full_model, bits=8):\n        self.full_model = full_model\n        self.bits = bits\n        self.scale = None\n        self.zero_point = None\n    \n    def quantize(self, example_input):\n        """Quantize the model for efficient inference"""\n        # Forward pass to get example outputs\n        with torch.no_grad():\n            full_output = self.full_model(example_input)\n        \n        # Compute quantization parameters\n        self.scale = (full_output.max() - full_output.min()) / (2**self.bits - 1)\n        self.zero_point = -full_output.min() / self.scale\n        \n        # Create quantized version of the model\n        self.quantized_model = torch.quantization.quantize_dynamic(\n            self.full_model, {nn.Linear}, dtype=torch.qint8\n        )\n    \n    def embed(self, input_data):\n        """Embed using quantized model"""\n        with torch.no_grad():\n            return self.quantized_model(input_data)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"continual-embedding-updates",children:"Continual Embedding Updates"}),"\n",(0,t.jsx)(n.p,{children:"For robots that continuously learn new concepts:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ContinualEmbedder:\n    def __init__(self, base_embedding_model):\n        self.model = base_embedding_model\n        self.exemplars = {}  # Store exemplars for replay\n        self.classifier = nn.Linear(512, 10)  # Initial 10 classes\n        self.new_class_idx = 10  # Index for new classes\n    \n    def add_new_concept(self, new_data, new_labels):\n        """Add new concepts while preventing forgetting"""\n        # Store exemplars of old concepts\n        with torch.no_grad():\n            old_embeddings = self.model.embed(new_data[:len(new_data)//2])\n            for i, emb in enumerate(old_embeddings):\n                class_id = new_labels[i]\n                if class_id not in self.exemplars:\n                    self.exemplars[class_id] = []\n                self.exemplars[class_id].append(emb.cpu())\n        \n        # Expand classifier for new classes\n        self.expand_classifier(len(set(new_labels)))\n        \n        # Fine-tune with replay of old concepts\n        self.finetune_with_replay(new_data, new_labels)\n    \n    def expand_classifier(self, num_new_classes):\n        """Expand the classifier for new classes"""\n        old_weight = self.classifier.weight.data\n        old_bias = self.classifier.bias.data\n        \n        in_features = self.classifier.in_features\n        old_out_features = self.classifier.out_features\n        new_out_features = old_out_features + num_new_classes\n        \n        # Create new classifier\n        new_classifier = nn.Linear(in_features, new_out_features)\n        \n        # Copy old weights and initialize new weights\n        with torch.no_grad():\n            new_classifier.weight[:old_out_features] = old_weight\n            new_classifier.bias[:old_out_features] = old_bias\n        \n        self.classifier = new_classifier\n    \n    def finetune_with_replay(self, new_data, new_labels):\n        """Fine-tune with replay of old exemplars"""\n        optimizer = torch.optim.Adam(list(self.model.parameters()) + \n                                   list(self.classifier.parameters()))\n        \n        # Combine new data with exemplars from memory\n        all_data = new_data\n        all_labels = new_labels\n        \n        for class_id, exemplars in self.exemplars.items():\n            # Add exemplars with their old labels\n            all_data.extend(exemplars)\n            all_labels.extend([class_id] * len(exemplars))\n        \n        # Training loop\n        for epoch in range(5):  # Few epochs to avoid catastrophic forgetting\n            for batch_data, batch_labels in self.create_batches(all_data, all_labels):\n                optimizer.zero_grad()\n                embeddings = self.model.embed(batch_data)\n                outputs = self.classifier(embeddings)\n                loss = F.cross_entropy(outputs, batch_labels)\n                loss.backward()\n                optimizer.step()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement a CLIP-based embedder that can match robotic action descriptions to relevant images in a dataset."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Design an embedding technique that combines visual, language, and robot state information for decision making."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Consider how you would optimize these embeddings for real-time robotics applications with limited computational resources."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The next section will explore how these embeddings are used for planning with Vision-Language Models in robotics."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);