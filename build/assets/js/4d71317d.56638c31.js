"use strict";(globalThis.webpackChunkeducational_ai_humanoid_robotics=globalThis.webpackChunkeducational_ai_humanoid_robotics||[]).push([[814],{7372:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"simulation/unity-robotics","title":"Unity Robotics Simulation for Humanoid Applications","description":"Overview","source":"@site/docs/simulation/unity-robotics.md","sourceDirName":"simulation","slug":"/simulation/unity-robotics","permalink":"/educational-ai-humanoid-robotics/docs/simulation/unity-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/educational-ai-humanoid-robotics/educational-ai-humanoid-robotics.github.io/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/unity-robotics.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim for Advanced Humanoid Robotics","permalink":"/educational-ai-humanoid-robotics/docs/simulation/isaac-sim"},"next":{"title":"Humanoid Robotics: Kinematics, Dynamics, and Control","permalink":"/educational-ai-humanoid-robotics/docs/humanoid-robotics/introduction"}}');var o=i(4848),r=i(8453);const a={sidebar_position:5},s="Unity Robotics Simulation for Humanoid Applications",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Architecture and Integration",id:"architecture-and-integration",level:2},{value:"ROS-TCP-Connector Integration",id:"ros-tcp-connector-integration",level:3},{value:"Physics Simulation in Unity",id:"physics-simulation-in-unity",level:2},{value:"Articulation Bodies for Humanoid Robots",id:"articulation-bodies-for-humanoid-robots",level:3},{value:"Perception and Sensor Simulation",id:"perception-and-sensor-simulation",level:2},{value:"Camera and Vision Sensors",id:"camera-and-vision-sensors",level:3},{value:"Human-Robot Interaction Features",id:"human-robot-interaction-features",level:2},{value:"VR/AR Support for Immersive Interfaces",id:"vrar-support-for-immersive-interfaces",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Multi-Resolution Simulation",id:"multi-resolution-simulation",level:3},{value:"Educational Applications",id:"educational-applications",level:2},{value:"Teaching Humanoid Robotics Concepts",id:"teaching-humanoid-robotics-concepts",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"unity-robotics-simulation-for-humanoid-applications",children:"Unity Robotics Simulation for Humanoid Applications"})}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"Unity Robotics provides a unique approach to robotics simulation, combining Unity's powerful game engine with robotics-specific tools. Unlike Gazebo and Isaac Sim, Unity excels in creating immersive, visually-rich environments that are particularly well-suited for human-robot interaction research, social robotics, and educational applications."}),"\n",(0,o.jsx)(e.h2,{id:"architecture-and-integration",children:"Architecture and Integration"}),"\n",(0,o.jsx)(e.h3,{id:"ros-tcp-connector-integration",children:"ROS-TCP-Connector Integration"}),"\n",(0,o.jsx)(e.p,{children:"Unity Robotics uses the ROS-TCP-Connector to enable communication between Unity and ROS/ROS2 systems:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor_msgs;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs;\nusing System.Collections.Generic;\n\npublic class UnityRobotController : MonoBehaviour\n{\n    [Header("ROS Configuration")]\n    public string rosIPAddress = "127.0.0.1";\n    public int rosPort = 10000;\n    \n    [Header("Robot Configuration")]\n    public string robotName = "UnityHumanoid";\n    public float controlFrequency = 100.0f; // Hz\n    public float simulationSpeed = 1.0f;\n    \n    // Robot joint control\n    [Header("Joint Configuration")]\n    public List<JointController> jointControllers;\n    \n    // Sensor simulation\n    [Header("Sensor Configuration")]\n    public List<SensorController> sensorControllers;\n    \n    private ROSConnection rosConnection;\n    private float controlTimer = 0f;\n    private float controlInterval;\n    \n    void Start()\n    {\n        // Initialize ROS connection\n        rosConnection = ROSConnection.GetOrCreateInstance();\n        rosConnection.Initialize(rosIPAddress, rosPort);\n        \n        // Calculate control interval based on frequency\n        controlInterval = 1.0f / controlFrequency;\n        \n        // Subscribe to ROS topics\n        SubscribeToTopics();\n        \n        // Publish initial state\n        InvokeRepeating("PublishRobotState", 0.0f, 0.01f); // 100 Hz for state publishing\n    }\n    \n    void SubscribeToTopics()\n    {\n        // Subscribe to joint commands\n        rosConnection.Subscribe<sensor_msgs.JointState>(\n            topic: $"/{robotName}/joint_commands",\n            callback: OnJointCommandReceived\n        );\n        \n        // Subscribe to velocity commands\n        rosConnection.Subscribe<geometry_msgs.Twist>(\n            topic: $"/{robotName}/cmd_vel",\n            callback: OnVelocityCommandReceived\n        );\n        \n        // Subscribe to other robot commands (grips, etc.)\n        rosConnection.Subscribe<std_msgs.String>(\n            topic: $"/{robotName}/behavior_command",\n            callback: OnBehaviorCommandReceived\n        );\n    }\n    \n    void Update()\n    {\n        // Execute control loop at specified frequency\n        controlTimer += Time.deltaTime;\n        if (controlTimer >= controlInterval)\n        {\n            ExecuteControlLoop();\n            controlTimer = 0f;\n        }\n    }\n    \n    void ExecuteControlLoop()\n    {\n        // Apply current joint commands to Unity articulation bodies\n        foreach (var jointCtrl in jointControllers)\n        {\n            if (jointCtrl.articulationBody != null)\n            {\n                ApplyJointControl(jointCtrl);\n            }\n        }\n        \n        // Process sensor data and publish to ROS\n        foreach (var sensorCtrl in sensorControllers)\n        {\n            sensorCtrl.UpdateAndPublish();\n        }\n    }\n    \n    void OnJointCommandReceived(sensor_msgs.JointState jointStateMsg)\n    {\n        // Process incoming joint commands\n        for (int i = 0; i < jointStateMsg.name.Count; i++)\n        {\n            string jointName = jointStateMsg.name[i];\n            \n            if (i < jointStateMsg.position.Count)\n            {\n                float targetPosition = (float)jointStateMsg.position[i];\n                \n                // Find corresponding joint controller\n                var jointCtrl = jointControllers.Find(jc => jc.jointName == jointName);\n                if (jointCtrl != null)\n                {\n                    jointCtrl.targetPosition = targetPosition;\n                }\n            }\n        }\n    }\n    \n    void OnVelocityCommandReceived(geometry_msgs.Twist twistMsg)\n    {\n        // Handle velocity commands for base movement\n        // This would typically affect the root/body of the humanoid\n        Vector3 linearVel = new Vector3(\n            (float)twistMsg.linear.x,\n            (float)twistMsg.linear.y, \n            (float)twistMsg.linear.z\n        );\n        \n        Vector3 angularVel = new Vector3(\n            (float)twistMsg.angular.x,\n            (float)twistMsg.angular.y,\n            (float)twistMsg.angular.z\n        );\n        \n        // Apply movement to robot base\n        ApplyBaseMovement(linearVel, angularVel);\n    }\n    \n    void OnBehaviorCommandReceived(std_msgs.String behaviorMsg)\n    {\n        // Handle behavior commands (greeting, waving, etc.)\n        StartCoroutine(ExecuteBehavior(behaviorMsg.data));\n    }\n    \n    void ApplyJointControl(JointController jointCtrl)\n    {\n        if (jointCtrl.articulationBody == null) return;\n        \n        // Get current joint state\n        ArticulationReducedSpacePosition currentPos = jointCtrl.articulationBody.jointPosition;\n        ArticulationReducedSpaceVelocity currentVel = jointCtrl.articulationBody.jointVelocity;\n        \n        // Calculate control action using PD controller\n        float positionError = jointCtrl.targetPosition - currentPos.x;\n        float velocityError = 0 - currentVel.x; // Assuming target velocity is 0\n        \n        float controlAction = jointCtrl.kp * positionError + jointCtrl.kd * velocityError;\n        \n        // Apply control (with safety limits)\n        controlAction = Mathf.Clamp(controlAction, -jointCtrl.maxForce, jointCtrl.maxForce);\n        \n        // Apply force/torque to joint\n        ArticulationDrive drive = jointCtrl.articulationBody.xDrive;\n        drive.target = jointCtrl.targetPosition * Mathf.Rad2Deg; // Convert to degrees for Unity\n        drive.stiffness = jointCtrl.kp;\n        drive.damping = jointCtrl.kd;\n        jointCtrl.articulationBody.xDrive = drive;\n    }\n    \n    void ApplyBaseMovement(Vector3 linearVel, Vector3 angularVel)\n    {\n        // Apply movement to the robot\'s root body\n        // This would be the main body/hip of the humanoid robot\n        Rigidbody rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            rb.velocity = linearVel;\n            rb.angularVelocity = angularVel;\n        }\n    }\n    \n    System.Collections.IEnumerator ExecuteBehavior(string behavior)\n    {\n        // Execute predefined behaviors\n        switch (behavior.ToLower())\n        {\n            case "wave":\n                yield return StartCoroutine(ExecuteWaveBehavior());\n                break;\n            case "greet":\n                yield return StartCoroutine(ExecuteGreetBehavior());\n                break;\n            case "dance":\n                yield return StartCoroutine(ExecuteDanceBehavior());\n                break;\n            default:\n                Debug.Log($"Unknown behavior: {behavior}");\n                break;\n        }\n    }\n    \n    System.Collections.IEnumerator ExecuteWaveBehavior()\n    {\n        // Example: Wave the right arm\n        var armJoint = jointControllers.Find(jc => jc.jointName == "right_shoulder_pitch");\n        if (armJoint != null)\n        {\n            float originalPosition = armJoint.targetPosition;\n            \n            // Move arm up\n            for (float t = 0; t < 1.0f; t += Time.deltaTime * 2.0f)\n            {\n                armJoint.targetPosition = Mathf.Lerp(originalPosition, 1.0f, t);\n                yield return null;\n            }\n            \n            // Wave motion\n            for (int i = 0; i < 3; i++)\n            {\n                // Wave up\n                for (float t = 0; t < 0.5f; t += Time.deltaTime * 4.0f)\n                {\n                    armJoint.targetPosition = 1.0f + Mathf.Sin(t * Mathf.PI * 4) * 0.2f;\n                    yield return null;\n                }\n                \n                // Wave down\n                for (float t = 0; t < 0.5f; t += Time.deltaTime * 4.0f)\n                {\n                    armJoint.targetPosition = 1.0f - Mathf.Sin(t * Mathf.PI * 4) * 0.2f;\n                    yield return null;\n                }\n            }\n            \n            // Return to original position\n            for (float t = 0; t < 1.0f; t += Time.deltaTime * 2.0f)\n            {\n                armJoint.targetPosition = Mathf.Lerp(1.0f, originalPosition, t);\n                yield return null;\n            }\n        }\n    }\n    \n    void PublishRobotState()\n    {\n        // Create and publish joint state message\n        var jointStateMsg = new sensor_msgs.JointState();\n        jointStateMsg.header = new std_msgs.Header();\n        jointStateMsg.header.stamp = new TimeStamp(rosConnection.GetServerTime());\n        jointStateMsg.header.frame_id = "base_link";\n        \n        foreach (var jointCtrl in jointControllers)\n        {\n            if (jointCtrl.articulationBody != null)\n            {\n                jointStateMsg.name.Add(jointCtrl.jointName);\n                \n                // Get current position in radians\n                ArticulationReducedSpacePosition pos = jointCtrl.articulationBody.jointPosition;\n                jointStateMsg.position.Add(pos.x);\n                \n                // Get current velocity\n                ArticulationReducedSpaceVelocity vel = jointCtrl.articulationBody.jointVelocity;\n                jointStateMsg.velocity.Add(vel.x);\n                \n                // Optionally add effort (calculated force/torque)\n                // jointStateMsg.effort.Add(calculatedEffort);\n            }\n        }\n        \n        rosConnection.Publish($"/{robotName}/joint_states", jointStateMsg);\n    }\n}\n\n[System.Serializable]\npublic class JointController\n{\n    public string jointName;\n    public ArticulationBody articulationBody;\n    [Range(0, 1000)] public float kp = 100f; // Proportional gain\n    [Range(0, 200)] public float kd = 20f;   // Derivative gain\n    [Range(0, 1000)] public float maxForce = 100f;\n    public float targetPosition = 0f;\n}\n\n[System.Serializable] \npublic class SensorController\n{\n    public string sensorName;\n    public string rosTopic;\n    public GameObject sensorObject;\n    \n    public virtual void UpdateAndPublish()\n    {\n        // Implementation will vary by sensor type\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"physics-simulation-in-unity",children:"Physics Simulation in Unity"}),"\n",(0,o.jsx)(e.h3,{id:"articulation-bodies-for-humanoid-robots",children:"Articulation Bodies for Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"Unity's Articulation Body system provides a physics-based approach to simulating articulated robots:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class HumanoidPhysicsSetup : MonoBehaviour\n{\n    [Header("Physical Properties")]\n    [Range(0.1f, 10.0f)] public float totalMass = 50.0f;\n    public float gravityScale = 1.0f;\n    \n    [Header("Joint Configuration")]\n    public ArticulationDriveConfig leftHipConfig;\n    public ArticulationDriveConfig rightHipConfig;\n    public ArticulationDriveConfig leftKneeConfig;\n    public ArticulationDriveConfig rightKneeConfig;\n    public ArticulationDriveConfig leftAnkleConfig;\n    public ArticulationDriveConfig rightAnkleConfig;\n    \n    void Start()\n    {\n        ConfigureHumanoidPhysics();\n    }\n    \n    void ConfigureHumanoidPhysics()\n    {\n        // Get all articulation bodies in the robot hierarchy\n        ArticulationBody[] allBodies = GetComponentsInChildren<ArticulationBody>();\n        \n        // Calculate mass distribution based on humanoid body proportions\n        AssignMassDistribution(allBodies);\n        \n        // Configure joints for humanoid movement\n        ConfigureJoints(allBodies);\n        \n        // Set up collision properties\n        ConfigureCollisions();\n    }\n    \n    void AssignMassDistribution(ArticulationBody[] bodies)\n    {\n        // Assign mass based on human body proportions\n        float totalCalculatedMass = 0f;\n        \n        foreach (var body in bodies)\n        {\n            string bodyName = body.name.ToLower();\n            \n            // Approximate mass distribution for a 50kg humanoid\n            float mass = 0f;\n            if (bodyName.Contains("head")) mass = 5.0f;        // 5kg for head\n            else if (bodyName.Contains("torso")) mass = 20.0f;  // 20kg for torso\n            else if (bodyName.Contains("thigh")) mass = 8.0f;   // 8kg for each thigh\n            else if (bodyName.Contains("shank")) mass = 4.0f;   // 4kg for each shank (lower leg)\n            else if (bodyName.Contains("foot")) mass = 1.5f;    // 1.5kg for each foot\n            else if (bodyName.Contains("upperarm")) mass = 3.0f; // 3kg for each upper arm\n            else if (bodyName.Contains("forearm")) mass = 2.0f;  // 2kg for each fore arm\n            else mass = 1.0f; // Default mass for other parts\n            \n            body.mass = mass;\n            totalCalculatedMass += mass;\n        }\n        \n        // Scale masses to match desired total mass\n        float scale = totalMass / totalCalculatedMass;\n        foreach (var body in bodies)\n        {\n            body.mass *= scale;\n        }\n    }\n    \n    void ConfigureJoints(ArticulationBody[] bodies)\n    {\n        foreach (var body in bodies)\n        {\n            string bodyName = body.name.ToLower();\n            \n            // Configure joint drives based on joint type and location\n            ArticulationDrive xDrive = body.xDrive;\n            ArticulationDrive yDrive = body.yDrive;\n            ArticulationDrive zDrive = body.zDrive;\n            \n            if (bodyName.Contains("hip"))\n            {\n                ConfigureJointForHip(body, ref xDrive, ref yDrive, ref zDrive);\n            }\n            else if (bodyName.Contains("knee"))\n            {\n                ConfigureJointForKnee(body, ref xDrive, ref yDrive, ref zDrive);\n            }\n            else if (bodyName.Contains("ankle"))\n            {\n                ConfigureJointForAnkle(body, ref xDrive, ref yDrive, ref zDrive);\n            }\n            else if (bodyName.Contains("shoulder"))\n            {\n                ConfigureJointForShoulder(body, ref xDrive, ref yDrive, ref zDrive);\n            }\n            else if (bodyName.Contains("elbow"))\n            {\n                ConfigureJointForElbow(body, ref xDrive, ref yDrive, ref zDrive);\n            }\n            \n            body.xDrive = xDrive;\n            body.yDrive = yDrive;\n            body.zDrive = zDrive;\n        }\n    }\n    \n    void ConfigureJointForHip(ArticulationBody body, ref ArticulationDrive x, ref ArticulationDrive y, ref ArticulationDrive z)\n    {\n        // Hip joint: typically 3 DOF (yaw, pitch, roll)\n        ConfigureDrive(ref x, 1000f, 100f, 3.14f, -3.14f); // Yaw\n        ConfigureDrive(ref y, 1000f, 100f, 1.57f, -0.5f);  // Pitch (allow forward bend)\n        ConfigureDrive(ref z, 800f, 80f, 0.5f, -0.5f);     // Roll (limited)\n    }\n    \n    void ConfigureJointForKnee(ArticulationBody body, ref ArticulationDrive x, ref ArticulationDrive y, ref ArticulationDrive z)\n    {\n        // Knee joint: primarily flexion\n        ConfigureDrive(ref x, 2000f, 200f, 2.5f, 0f);  // Flexion only (0 to ~143 degrees)\n        ConfigureDrive(ref y, 0f, 0f, 0f, 0f);          // Locked in other axes\n        ConfigureDrive(ref z, 0f, 0f, 0f, 0f);\n    }\n    \n    void ConfigureJointForAnkle(ArticulationBody body, ref ArticulationDrive x, ref ArticulationDrive y, ref ArticulationDrive z)\n    {\n        // Ankle joint: pitch and limited roll\n        ConfigureDrive(ref x, 500f, 50f, 0.5f, -0.5f);  // Pitch (dorsiflexion/plantarflexion)\n        ConfigureDrive(ref y, 0f, 0f, 0f, 0f);           // Typically locked\n        ConfigureDrive(ref z, 300f, 30f, 0.3f, -0.3f);   // Roll (inversion/eversion)\n    }\n    \n    void ConfigureJointForShoulder(ArticulationBody body, ref ArticulationDrive x, ref ArticulationDrive y, ref ArticulationDrive z)\n    {\n        // Shoulder joint: 3 DOF with significant range\n        ConfigureDrive(ref x, 500f, 50f, 1.57f, -1.57f); // Yaw\n        ConfigureDrive(ref y, 500f, 50f, 2.5f, -0.5f);   // Pitch\n        ConfigureDrive(ref z, 500f, 50f, 1.57f, -1.57f); // Roll\n    }\n    \n    void ConfigureJointForElbow(ArticulationBody body, ref ArticulationDrive x, ref ArticulationDrive y, ref ArticulationDrive z)\n    {\n        // Elbow joint: primarily flexion\n        ConfigureDrive(ref x, 1000f, 100f, 2.5f, 0f);   // Flexion only\n        ConfigureDrive(ref y, 0f, 0f, 0f, 0f);           // Locked\n        ConfigureDrive(ref z, 0f, 0f, 0f, 0f);\n    }\n    \n    void ConfigureDrive(ref ArticulationDrive drive, float stiffness, float damping, float upperLimit, float lowerLimit)\n    {\n        drive.stiffness = stiffness;\n        drive.damping = damping;\n        drive.forceLimit = 1000f; // Maximum force (N)\n        drive.lowerLimit = lowerLimit;\n        drive.upperLimit = upperLimit;\n        drive.hasLimits = true;\n    }\n    \n    void ConfigureCollisions()\n    {\n        // Add appropriate colliders to prevent self-intersection\n        // This is typically done through Unity editor or automatically via URDF import\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"perception-and-sensor-simulation",children:"Perception and Sensor Simulation"}),"\n",(0,o.jsx)(e.h3,{id:"camera-and-vision-sensors",children:"Camera and Vision Sensors"}),"\n",(0,o.jsx)(e.p,{children:"Unity's rendering pipeline can be leveraged for sophisticated vision sensor simulation:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor_msgs;\nusing System.Collections;\nusing System.Collections.Generic;\n\npublic class UnityVisionSensor : SensorController\n{\n    [Header("Camera Configuration")]\n    public Camera sensorCamera;\n    public int imageWidth = 640;\n    public int imageHeight = 480;\n    public float updateRate = 30.0f; // Hz\n    \n    [Header("Sensor Properties")]\n    public bool enableDepth = true;\n    public bool enableSemanticSegmentation = false;\n    public float minDepth = 0.1f;\n    public float maxDepth = 10.0f;\n    \n    private float updateInterval;\n    private float lastUpdateTime;\n    private RenderTexture renderTexture;\n    private ROSConnection rosConnection;\n    \n    void Start()\n    {\n        rosConnection = ROSConnection.GetOrCreateInstance();\n        \n        // Calculate update interval\n        updateInterval = 1.0f / updateRate;\n        \n        // Create render texture for camera\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        sensorCamera.targetTexture = renderTexture;\n    }\n    \n    public override void UpdateAndPublish()\n    {\n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            PublishImage();\n            lastUpdateTime = Time.time;\n        }\n    }\n    \n    void PublishImage()\n    {\n        // Capture image from camera\n        Texture2D image = CaptureImageFromCamera();\n        \n        // Create ROS Image message\n        var rosImage = new sensor_msgs.Image();\n        \n        // Fill header\n        rosImage.header = new std_msgs.Header();\n        rosImage.header.stamp = new TimeStamp(rosConnection.GetServerTime());\n        rosImage.header.frame_id = sensorName + "_optical_frame";\n        \n        // Fill image properties\n        rosImage.height = (uint)imageHeight;\n        rosImage.width = (uint)imageWidth;\n        rosImage.encoding = "rgb8"; // For RGB images\n        rosImage.is_bigendian = 0;\n        rosImage.step = (uint)(imageWidth * 3); // 3 bytes per pixel for RGB\n        \n        // Convert texture data to byte array\n        byte[] imageData = image.GetRawTextureData<byte>();\n        rosImage.data = new List<byte>(imageData);\n        \n        // Publish the image\n        rosConnection.Publish(rosTopic, rosImage);\n        \n        // Clean up\n        Destroy(image);\n    }\n    \n    Texture2D CaptureImageFromCamera()\n    {\n        // Create a temporary RenderTexture to read the camera output\n        RenderTexture currentRT = RenderTexture.active;\n        RenderTexture.active = renderTexture;\n        \n        Texture2D image = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n        image.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        image.Apply();\n        \n        RenderTexture.active = currentRT;\n        return image;\n    }\n    \n    // Method for depth image capture (simplified)\n    public Texture2D CaptureDepthImage()\n    {\n        // In a real implementation, this would render depth information\n        // to the camera and read it as a grayscale image\n        Texture2D depthImage = new Texture2D(imageWidth, imageHeight, TextureFormat.RFloat, false);\n        \n        // This is a placeholder - in practice, you\'d render depth to this texture\n        // using a custom shader that encodes depth information\n        \n        return depthImage;\n    }\n}\n\npublic class UnityMultiModalSensor : MonoBehaviour\n{\n    [Header("Multi-Modal Sensor Setup")]\n    public List<UnityVisionSensor> visionSensors;\n    public UnityIMUSensor imuSensor;\n    public UnityForceTorqueSensor forceTorqueSensor;\n    \n    [Header("Sensor Fusion Configuration")]\n    public bool enableSensorFusion = true;\n    \n    void Start()\n    {\n        InitializeSensors();\n    }\n    \n    void InitializeSensors()\n    {\n        // Initialize all vision sensors\n        foreach (var visionSensor in visionSensors)\n        {\n            visionSensor.enabled = true;\n        }\n        \n        // Initialize other sensors\n        if (imuSensor != null)\n            imuSensor.enabled = true;\n            \n        if (forceTorqueSensor != null)\n            forceTorqueSensor.enabled = true;\n    }\n    \n    void Update()\n    {\n        if (enableSensorFusion)\n        {\n            ProcessSensorFusion();\n        }\n    }\n    \n    void ProcessSensorFusion()\n    {\n        // In a real implementation, this would combine data from multiple sensors\n        // to create a more comprehensive perception of the environment\n        \n        // Example: Combine camera and depth information for 3D object detection\n        // Example: Fuse IMU data with vision for better pose estimation\n        // Example: Combine force/torque with vision for manipulation planning\n    }\n}\n\npublic class UnityIMUSensor : SensorController\n{\n    [Header("IMU Configuration")]\n    public float updateRate = 200.0f; // Hz\n    public Vector3 noiseLevel = new Vector3(0.01f, 0.01f, 0.01f);\n    \n    private float updateInterval;\n    private float lastUpdateTime;\n    private Rigidbody attachedRigidbody;\n    \n    void Start()\n    {\n        updateInterval = 1.0f / updateRate;\n        attachedRigidbody = GetComponent<Rigidbody>();\n        if (attachedRigidbody == null)\n            attachedRigidbody = GetComponentInParent<Rigidbody>();\n    }\n    \n    public override void UpdateAndPublish()\n    {\n        if (attachedRigidbody == null) return;\n        \n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            PublishIMUData();\n            lastUpdateTime = Time.time;\n        }\n    }\n    \n    void PublishIMUData()\n    {\n        // Create IMU message\n        var imuMsg = new sensor_msgs.Imu();\n        imuMsg.header = new std_msgs.Header();\n        imuMsg.header.stamp = new TimeStamp(\n            Unity.Robotics.ROSTCPConnector.ROSConnection.GetOrCreateInstance().GetServerTime()\n        );\n        imuMsg.header.frame_id = sensorName + "_link";\n        \n        // Fill orientation from Unity rotation\n        // Convert from Unity coordinates to ROS coordinates\n        Quaternion unityRotation = transform.rotation;\n        imuMsg.orientation.x = unityRotation.x;\n        imuMsg.orientation.y = unityRotation.z;  // Unity Z becomes ROS Y\n        imuMsg.orientation.z = unityRotation.y;  // Unity Y becomes ROS Z\n        imuMsg.orientation.w = unityRotation.w;\n        \n        // Fill angular velocity\n        Vector3 angularVel = attachedRigidbody.angularVelocity;\n        imuMsg.angular_velocity.x = angularVel.x;\n        imuMsg.angular_velocity.y = angularVel.z;  // Unity Z becomes ROS Y\n        imuMsg.angular_velocity.z = angularVel.y;  // Unity Y becomes ROS Z\n        \n        // Fill linear acceleration\n        Vector3 linearAcc = attachedRigidbody.velocity; // Simplified - should differentiate\n        imuMsg.linear_acceleration.x = linearAcc.x;\n        imuMsg.linear_acceleration.y = linearAcc.z;  // Unity Z becomes ROS Y\n        imuMsg.linear_acceleration.z = linearAcc.y;  // Unity Y becomes ROS Z\n        \n        // Add noise to simulate real sensor\n        AddNoiseToIMU(ref imuMsg);\n        \n        // Publish IMU data\n        var rosConnection = Unity.Robotics.ROSTCPConnector.ROSConnection.GetOrCreateInstance();\n        rosConnection.Publish(rosTopic, imuMsg);\n    }\n    \n    void AddNoiseToIMU(ref sensor_msgs.Imu imuMsg)\n    {\n        System.Random rand = new System.Random();\n        \n        // Add Gaussian noise to measurements\n        float noiseX = (float)(rand.NextDouble() - 0.5) * noiseLevel.x;\n        float noiseY = (float)(rand.NextDouble() - 0.5) * noiseLevel.y;\n        float noiseZ = (float)(rand.NextDouble() - 0.5) * noiseLevel.z;\n        \n        imuMsg.orientation.x += noiseX;\n        imuMsg.orientation.y += noiseY;\n        imuMsg.orientation.z += noiseZ;\n        \n        imuMsg.angular_velocity.x += noiseX;\n        imuMsg.angular_velocity.y += noiseY;\n        imuMsg.angular_velocity.z += noiseZ;\n        \n        imuMsg.linear_acceleration.x += noiseX;\n        imuMsg.linear_acceleration.y += noiseY;\n        imuMsg.linear_acceleration.z += noiseZ;\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"human-robot-interaction-features",children:"Human-Robot Interaction Features"}),"\n",(0,o.jsx)(e.h3,{id:"vrar-support-for-immersive-interfaces",children:"VR/AR Support for Immersive Interfaces"}),"\n",(0,o.jsx)(e.p,{children:"Unity's strength in VR/AR makes it ideal for creating immersive human-robot interfaces:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.XR;\n\npublic class HumanRobotInterfaceVR : MonoBehaviour\n{\n    [Header("VR Configuration")]\n    public Transform playerHead;\n    public Transform leftController;\n    public Transform rightController;\n    \n    [Header("Interaction Parameters")]\n    public float interactionDistance = 3.0f;\n    public LayerMask interactionLayers;\n    \n    [Header("Social Interaction")]\n    public float personalSpaceRadius = 1.5f;\n    public float attentionDistance = 5.0f;\n    \n    private UnityRobotController robotController;\n    private bool userDetected = false;\n    private float lastGreetingTime = 0f;\n    private float greetingCooldown = 10.0f; // seconds\n    \n    void Start()\n    {\n        robotController = FindObjectOfType<UnityRobotController>();\n        \n        // Initialize VR components\n        InitializeVRTracking();\n    }\n    \n    void Update()\n    {\n        DetectAndRespondToUser();\n        UpdateSocialBehaviors();\n    }\n    \n    void InitializeVRTracking()\n    {\n        // The playerHead, leftController, and rightController should be\n        // set up by your VR/AR SDK (Oculus, OpenXR, etc.)\n    }\n    \n    void DetectAndRespondToUser()\n    {\n        if (playerHead == null) return;\n        \n        // Calculate distance to user\n        float distanceToUser = Vector3.Distance(transform.position, playerHead.position);\n        \n        // Update robot behavior based on user proximity\n        if (distanceToUser < attentionDistance)\n        {\n            userDetected = true;\n            \n            // Trigger greeting if enough time has passed\n            if (distanceToUser < interactionDistance && \n                Time.time - lastGreetingTime > greetingCooldown)\n            {\n                TriggerGreeting();\n            }\n            \n            // Adjust robot behavior based on distance\n            AdjustBehaviorForDistance(distanceToUser);\n        }\n        else\n        {\n            userDetected = false;\n        }\n    }\n    \n    void AdjustBehaviorForDistance(float distance)\n    {\n        if (robotController == null) return;\n        \n        if (distance < personalSpaceRadius)\n        {\n            // If user is in personal space, robot should be more cautious\n            var rosConnection = Unity.Robotics.ROSTCPConnector.ROSConnection.GetOrCreateInstance();\n            var retreatMsg = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs.String();\n            retreatMsg.data = "personal_space_respected";\n            rosConnection.Publish("/" + robotController.robotName + "/behavior_command", retreatMsg);\n        }\n        else if (distance < interactionDistance)\n        {\n            // In interaction range, robot should show attention\n            var rosConnection = Unity.Robotics.ROSTCPConnector.ROSConnection.GetOrCreateInstance();\n            var attentionMsg = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs.String();\n            attentionMsg.data = "paying_attention";\n            rosConnection.Publish("/" + robotController.robotName + "/behavior_command", attentionMsg);\n        }\n    }\n    \n    void TriggerGreeting()\n    {\n        if (robotController == null) return;\n        \n        var rosConnection = Unity.Robotics.ROSTCPConnector.ROSConnection.GetOrCreateInstance();\n        var greetingMsg = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs.String();\n        greetingMsg.data = "greeting";\n        rosConnection.Publish("/" + robotController.robotName + "/behavior_command", greetingMsg);\n        \n        lastGreetingTime = Time.time;\n    }\n    \n    void UpdateSocialBehaviors()\n    {\n        if (!userDetected || playerHead == null) return;\n        \n        // Make robot look at user\n        Vector3 directionToUser = (playerHead.position - transform.position).normalized;\n        float angleToUser = Vector3.Angle(transform.forward, directionToUser);\n        \n        if (angleToUser > 10f) // Only turn if significantly not facing user\n        {\n            // Rotate robot to face user\n            Vector3 targetDirection = new Vector3(directionToUser.x, 0, directionToUser.z).normalized;\n            if (targetDirection != Vector3.zero)\n            {\n                transform.rotation = Quaternion.LookRotation(targetDirection, Vector3.up);\n            }\n        }\n        \n        // Handle controller-based interactions\n        HandleControllerInteractions();\n    }\n    \n    void HandleControllerInteractions()\n    {\n        // Handle gestures or pointing from VR controllers\n        if (leftController != null && rightController != null)\n        {\n            // Check if user is pointing at the robot with either controller\n            CheckControllerPointing(leftController, "left");\n            CheckControllerPointing(rightController, "right");\n        }\n    }\n    \n    void CheckControllerPointing(Transform controller, string controllerName)\n    {\n        RaycastHit hit;\n        Vector3 controllerForward = controller.TransformDirection(Vector3.forward);\n        \n        if (Physics.Raycast(controller.position, controllerForward, out hit, interactionDistance, interactionLayers))\n        {\n            if (hit.collider.CompareTag("RobotPart") || hit.collider.transform.IsChildOf(transform))\n            {\n                // User is pointing at robot - trigger attention behavior\n                TriggerPointedAtBehavior(controllerName);\n            }\n        }\n    }\n    \n    void TriggerPointedAtBehavior(string controllerName)\n    {\n        // Robot acknowledges being pointed at\n        var rosConnection = Unity.Robotics.ROSTCPConnector.ROSConnection.GetOrCreateInstance();\n        var acknowledgmentMsg = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs.String();\n        acknowledgmentMsg.data = $"acknowledged_pointing_from_{controllerName}";\n        rosConnection.Publish("/" + robotController.robotName + "/behavior_command", acknowledgmentMsg);\n    }\n}\n\npublic class SocialBehaviorController : MonoBehaviour\n{\n    [Header("Behavior Configuration")]\n    public float attentionThreshold = 5.0f;\n    public float interactionDistance = 2.0f;\n    public float personalSpaceDistance = 1.5f;\n    \n    [Header("Animation Triggers")]\n    public string attentionAnimation = "pay_attention";\n    public string greetingAnimation = "greeting";\n    public string retreatAnimation = "step_back";\n    public string idleAnimation = "idle";\n    \n    private Animator animator;\n    private bool isInPersonalSpace = false;\n    private bool isAttentive = false;\n    \n    void Start()\n    {\n        animator = GetComponent<Animator>();\n    }\n    \n    public void UpdateSocialBehavior(Vector3 userPosition)\n    {\n        float distance = Vector3.Distance(transform.position, userPosition);\n        \n        // Check personal space violation\n        if (distance < personalSpaceDistance && !isInPersonalSpace)\n        {\n            isInPersonalSpace = true;\n            TriggerRetreatBehavior();\n        }\n        else if (distance > personalSpaceDistance && isInPersonalSpace)\n        {\n            isInPersonalSpace = false;\n            TriggerIdleBehavior();\n        }\n        \n        // Check for attention and interaction\n        if (distance < attentionThreshold && distance > personalSpaceDistance)\n        {\n            if (!isAttentive)\n            {\n                isAttentive = true;\n                TriggerAttentionBehavior();\n            }\n            \n            // Check for close interaction\n            if (distance < interactionDistance)\n            {\n                TriggerGreetingBehavior();\n            }\n        }\n        else if (isAttentive)\n        {\n            isAttentive = false;\n            TriggerIdleBehavior();\n        }\n    }\n    \n    void TriggerAttentionBehavior()\n    {\n        if (animator != null)\n        {\n            animator.SetBool(attentionAnimation, true);\n            animator.SetBool(idleAnimation, false);\n        }\n    }\n    \n    void TriggerGreetingBehavior()\n    {\n        if (animator != null)\n        {\n            animator.SetTrigger(greetingAnimation);\n        }\n    }\n    \n    void TriggerRetreatBehavior()\n    {\n        if (animator != null)\n        {\n            animator.SetTrigger(retreatAnimation);\n        }\n    }\n    \n    void TriggerIdleBehavior()\n    {\n        if (animator != null)\n        {\n            animator.SetBool(attentionAnimation, false);\n            animator.SetBool(idleAnimation, true);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"multi-resolution-simulation",children:"Multi-Resolution Simulation"}),"\n",(0,o.jsx)(e.p,{children:"Unity allows for multi-resolution simulation approaches to balance quality and performance:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class MultiResolutionSimulator : MonoBehaviour\n{\n    [Header("Simulation Optimization")]\n    [Range(0.1f, 1.0f)] public float highDetailRange = 10.0f;\n    [Range(10.0f, 100.0f)] public float lowDetailRange = 50.0f;\n    \n    [Header("Detail Levels")]\n    public int highDetailLOD = 0;\n    public int mediumDetailLOD = 1;\n    public int lowDetailLOD = 2;\n    \n    private List<SimulatedObject> simulatedObjects = new List<SimulatedObject>();\n    private Transform viewerTransform; // Camera or main viewpoint\n    \n    void Start()\n    {\n        viewerTransform = Camera.main.transform;\n        InitializeSimulatedObjects();\n    }\n    \n    void Update()\n    {\n        UpdateObjectDetailLevels();\n    }\n    \n    void InitializeSimulatedObjects()\n    {\n        // Find all objects that participate in simulation\n        GameObject[] allObjects = GameObject.FindGameObjectsWithTag("Simulatable");\n        \n        foreach (GameObject obj in allObjects)\n        {\n            SimulatedObject simObj = new SimulatedObject();\n            simObj.gameObject = obj;\n            simObj.originalLOD = 0; // Set to highest detail initially\n            simObj.currentLOD = 0;\n            simulatedObjects.Add(simObj);\n        }\n    }\n    \n    void UpdateObjectDetailLevels()\n    {\n        if (viewerTransform == null) return;\n        \n        foreach (SimulatedObject simObj in simulatedObjects)\n        {\n            if (simObj.gameObject == null) continue;\n            \n            float distance = Vector3.Distance(viewerTransform.position, simObj.gameObject.transform.position);\n            \n            int targetLOD;\n            if (distance < highDetailRange)\n            {\n                targetLOD = highDetailLOD;\n            }\n            else if (distance < lowDetailRange)\n            {\n                targetLOD = mediumDetailLOD;\n            }\n            else\n            {\n                targetLOD = lowDetailLOD;\n            }\n            \n            // Only apply changes if LOD level changed\n            if (targetLOD != simObj.currentLOD)\n            {\n                SetObjectLOD(simObj, targetLOD);\n                simObj.currentLOD = targetLOD;\n            }\n        }\n    }\n    \n    void SetObjectLOD(SimulatedObject simObj, int lodLevel)\n    {\n        if (simObj.meshRenderer != null)\n        {\n            // Adjust renderer settings based on LOD\n            switch (lodLevel)\n            {\n                case 0: // High detail\n                    simObj.meshRenderer.shadowCastingMode = UnityEngine.Rendering.ShadowCastingMode.On;\n                    simObj.meshRenderer.receiveShadows = true;\n                    break;\n                case 1: // Medium detail\n                    simObj.meshRenderer.shadowCastingMode = UnityEngine.Rendering.ShadowCastingMode.TwoSided;\n                    simObj.meshRenderer.receiveShadows = false;\n                    break;\n                case 2: // Low detail\n                    simObj.meshRenderer.shadowCastingMode = UnityEngine.Rendering.ShadowCastingMode.Off;\n                    simObj.meshRenderer.receiveShadows = false;\n                    break;\n            }\n        }\n        \n        // Adjust physics simulation if applicable\n        if (simObj.articulationBody != null)\n        {\n            // For distant objects, we might reduce physics update frequency\n            // or use simplified collision shapes\n        }\n    }\n}\n\n[System.Serializable]\npublic class SimulatedObject\n{\n    public GameObject gameObject;\n    public MeshRenderer meshRenderer;\n    public ArticulationBody articulationBody;\n    public int originalLOD;\n    public int currentLOD;\n    \n    public SimulatedObject()\n    {\n        // Initialize with default values\n        originalLOD = 0;\n        currentLOD = 0;\n    }\n}\n\npublic class PhysicsOptimizationManager : MonoBehaviour\n{\n    [Header("Physics Optimization")]\n    public float farPhysicsDistance = 20.0f;\n    public float simulationSpeedScale = 1.0f;\n    \n    private List<ArticulationBody> farArticulationBodies = new List<ArticulationBody>();\n    \n    void Start()\n    {\n        // Find all articulation bodies in the scene\n        ArticulationBody[] allBodies = FindObjectsOfType<ArticulationBody>();\n        \n        foreach (ArticulationBody body in allBodies)\n        {\n            if (Vector3.Distance(transform.position, body.transform.position) > farPhysicsDistance)\n            {\n                farArticulationBodies.Add(body);\n            }\n        }\n    }\n    \n    void Update()\n    {\n        // Reduce simulation quality for distant objects\n        Camera mainCam = Camera.main;\n        if (mainCam != null)\n        {\n            foreach (ArticulationBody body in farArticulationBodies)\n            {\n                float distanceToCamera = Vector3.Distance(mainCam.transform.position, body.transform.position);\n                \n                if (distanceToCamera > farPhysicsDistance)\n                {\n                    // Reduce solver iterations for distant bodies\n                    // This requires Unity Physics package customization\n                    // For now, this is conceptual\n                    ReducePhysicsQualityForBody(body, distanceToCamera);\n                }\n            }\n        }\n    }\n    \n    void ReducePhysicsQualityForBody(ArticulationBody body, float distance)\n    {\n        // Conceptual implementation for reducing physics quality\n        // Based on distance from camera\n        float qualityFactor = Mathf.Clamp01(1.0f - (distance - farPhysicsDistance) / 10.0f);\n        \n        // In a real implementation, you would adjust:\n        // - Solver iterations\n        // - Collision detection frequency\n        // - Joint constraint quality\n        // This requires a custom physics pipeline\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"educational-applications",children:"Educational Applications"}),"\n",(0,o.jsx)(e.h3,{id:"teaching-humanoid-robotics-concepts",children:"Teaching Humanoid Robotics Concepts"}),"\n",(0,o.jsx)(e.p,{children:"Unity's visual nature makes it excellent for educational purposes:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\n\npublic class EducationalHumanoidInterface : MonoBehaviour\n{\n    [Header("Educational UI")]\n    public Text robotStatusText;\n    public Text jointStatusText;\n    public Text lessonStatusText;\n    public Text physicsExplanationText;\n    public Button[] lessonButtons;\n    \n    [Header("Educational Content")]\n    public EducationalLesson[] lessons;\n    public int currentLesson = 0;\n    \n    private UnityRobotController robotController;\n    private UnityPhysicsSetup physicsSetup;\n    \n    void Start()\n    {\n        robotController = FindObjectOfType<UnityRobotController>();\n        physicsSetup = FindObjectOfType<UnityPhysicsSetup>();\n        InitializeLessonInterface();\n        LoadLesson(currentLesson);\n    }\n    \n    void InitializeLessonInterface()\n    {\n        // Setup lesson buttons\n        for (int i = 0; i < lessonButtons.Length && i < lessons.Length; i++)\n        {\n            int lessonIndex = i; // Capture for closure\n            lessonButtons[i].onClick.AddListener(() => LoadLesson(lessonIndex));\n            lessonButtons[i].GetComponentInChildren<Text>().text = lessons[i].title;\n        }\n    }\n    \n    public void LoadLesson(int lessonIndex)\n    {\n        if (lessonIndex >= 0 && lessonIndex < lessons.Length)\n        {\n            EducationalLesson lesson = lessons[lessonIndex];\n            currentLesson = lessonIndex;\n            \n            // Update UI\n            lessonStatusText.text = $"Lesson {lessonIndex + 1}: {lesson.title}";\n            physicsExplanationText.text = lesson.physicsExplanation;\n            \n            // Configure robot for this lesson\n            ConfigureRobotForLesson(lesson);\n            \n            // Show lesson-specific controls\n            ShowLessonControls(lesson);\n        }\n    }\n    \n    void ConfigureRobotForLesson(EducationalLesson lesson)\n    {\n        // Configure the robot based on lesson requirements\n        switch (lesson.topic)\n        {\n            case LessonTopic.Balance:\n                ConfigureBalanceLesson();\n                break;\n            case LessonTopic.Walking:\n                ConfigureWalkingLesson();\n                break;\n            case LessonTopic.Manipulation:\n                ConfigureManipulationLesson();\n                break;\n            case LessonTopic.Kinematics:\n                ConfigureKinematicsLesson();\n                break;\n        }\n    }\n    \n    void ConfigureBalanceLesson()\n    {\n        // Setup robot in balance scenario\n        // Add balance challenges, visual aids, etc.\n    }\n    \n    void ConfigureWalkingLesson()\n    {\n        // Setup robot in walking scenario\n        // Add terrain challenges, walking patterns, etc.\n    }\n    \n    void ConfigureManipulationLesson()\n    {\n        // Setup robot in manipulation scenario\n        // Add objects to manipulate, challenges, etc.\n    }\n    \n    void ConfigureKinematicsLesson()\n    {\n        // Setup robot for kinematics demonstration\n        // Add joint controls, visualization, etc.\n    }\n    \n    void ShowLessonControls(EducationalLesson lesson)\n    {\n        // Show/hide controls based on lesson type\n        // This would update the UI to show relevant controls\n    }\n    \n    void Update()\n    {\n        // Update status displays\n        UpdateRobotStatus();\n    }\n    \n    void UpdateRobotStatus()\n    {\n        if (robotController != null)\n        {\n            // Update status text with current robot information\n            if (robotController.jointControllers != null)\n            {\n                string jointInfo = "";\n                foreach (var jointCtrl in robotController.jointControllers)\n                {\n                    if (jointCtrl.articulationBody != null)\n                    {\n                        jointInfo += $"{jointCtrl.jointName}: {jointCtrl.targetPosition:F2} ";\n                    }\n                }\n                jointStatusText.text = "Joints: " + jointInfo;\n            }\n            \n            // Update other status information\n            robotStatusText.text = "Status: Running";\n        }\n    }\n    \n    public void ExecuteLessonTask(string task)\n    {\n        // Execute a specific task related to the current lesson\n        switch (task)\n        {\n            case "balance_challenge":\n                // Execute balance challenge\n                break;\n            case "walk_forward":\n                // Execute walking command\n                break;\n            case "wave_hello":\n                // Execute greeting behavior\n                StartCoroutine(robotController.ExecuteBehavior("wave"));\n                break;\n        }\n    }\n}\n\n[System.Serializable]\npublic class EducationalLesson\n{\n    public string title;\n    public LessonTopic topic;\n    public string physicsExplanation;\n    public string[] learningObjectives;\n    public string[] requiredEquipment;\n    public LessonActivity[] activities;\n}\n\npublic enum LessonTopic\n{\n    Balance,\n    Walking,\n    Manipulation,\n    Kinematics,\n    Dynamics,\n    Control,\n    Perception,\n    SocialInteraction\n}\n\n[System.Serializable]\npublic class LessonActivity\n{\n    public string activityTitle;\n    public string instructions;\n    public string successCriteria;\n    public float timeEstimate;\n}\n\n// Example lesson definition\npublic class ExampleLesson : MonoBehaviour\n{\n    void Start()\n    {\n        // This would define the actual lesson content\n        EducationalLesson balanceLesson = new EducationalLesson\n        {\n            title = "Understanding Balance in Humanoid Robots",\n            topic = LessonTopic.Balance,\n            physicsExplanation = "Balance in humanoid robots is maintained by keeping the center of mass over the support polygon formed by the feet...",\n            learningObjectives = new string[] {\n                "Understand the concept of center of mass",\n                "Learn how joint control affects balance",\n                "Experience the challenge of bipedal balance"\n            },\n            requiredEquipment = new string[] { "Humanoid robot model", "Balance challenge environment" },\n            activities = new LessonActivity[] {\n                new LessonActivity {\n                    activityTitle = "Center of Mass Visualization",\n                    instructions = "Observe how the center of mass moves when you change joint angles",\n                    successCriteria = "Student can identify when the robot is balanced vs unbalanced",\n                    timeEstimate = 15.0f\n                }\n            }\n        };\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:"Using the educational AI agents, design a Unity simulation environment where a humanoid robot learns to navigate around obstacles while maintaining balance."}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:"Implement a sensor fusion system that combines data from multiple sensors to improve the robot's perception of its environment."}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:"Consider how Unity's VR capabilities could enhance the educational experience for humanoid robotics concepts."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"The next section will explore how to integrate simulation results with real robot systems and best practices for sim-to-real transfer in Unity Robotics."})]})}function u(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);