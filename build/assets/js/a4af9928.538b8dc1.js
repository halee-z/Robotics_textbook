"use strict";(globalThis.webpackChunkeducational_ai_humanoid_robotics=globalThis.webpackChunkeducational_ai_humanoid_robotics||[]).push([[817],{128:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"projects/project2","title":"Getting Started with Projects","description":"Overview","source":"@site/docs/projects/project2.md","sourceDirName":"projects","slug":"/projects/project2","permalink":"/educational-ai-humanoid-robotics/docs/projects/project2","draft":false,"unlisted":false,"editUrl":"https://github.com/educational-ai-humanoid-robotics/educational-ai-humanoid-robotics.github.io/tree/main/packages/create-docusaurus/templates/shared/docs/projects/project2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Project 1: Design and Simulate a Simple Humanoid Walker","permalink":"/educational-ai-humanoid-robotics/docs/projects/project1"},"next":{"title":"Project 3: Socially Assistive Robot for Elderly Care","permalink":"/educational-ai-humanoid-robotics/docs/projects/project3"}}');var s=t(4848),i=t(8453);const r={sidebar_position:3},a="Getting Started with Projects",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Project Structure",id:"project-structure",level:2},{value:"Project 1: Autonomous Object Retrieval System",id:"project-1-autonomous-object-retrieval-system",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Step 1: Environment Setup",id:"step-1-environment-setup",level:4},{value:"Step 2: Perception System",id:"step-2-perception-system",level:4},{value:"Step 3: Navigation System",id:"step-3-navigation-system",level:4},{value:"Step 4: Manipulation System",id:"step-4-manipulation-system",level:4},{value:"Step 5: System Integration",id:"step-5-system-integration",level:4},{value:"Testing the Project",id:"testing-the-project",level:3},{value:"Extensions",id:"extensions",level:3},{value:"Project 2: Humanoid Robot Dance Choreography",id:"project-2-humanoid-robot-dance-choreography",level:2},{value:"Learning Objectives",id:"learning-objectives-1",level:3},{value:"Prerequisites",id:"prerequisites-1",level:3},{value:"Implementation Steps",id:"implementation-steps-1",level:3},{value:"Step 1: Dance Motion Primitives",id:"step-1-dance-motion-primitives",level:4},{value:"Step 2: Choreography Sequencer",id:"step-2-choreography-sequencer",level:4},{value:"Testing the Project",id:"testing-the-project-1",level:3},{value:"Extensions",id:"extensions-1",level:3},{value:"Project 3: Educational Tutor Robot",id:"project-3-educational-tutor-robot",level:2},{value:"Learning Objectives",id:"learning-objectives-2",level:3},{value:"Prerequisites",id:"prerequisites-2",level:3},{value:"Implementation Steps",id:"implementation-steps-2",level:3},{value:"Step 1: Student Model Tracker",id:"step-1-student-model-tracker",level:4},{value:"Step 2: Adaptive Tutor Controller",id:"step-2-adaptive-tutor-controller",level:4},{value:"Step 3: Main Tutoring System",id:"step-3-main-tutoring-system",level:4},{value:"Testing the Project",id:"testing-the-project-2",level:3},{value:"Extensions",id:"extensions-2",level:3},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"getting-started-with-projects",children:"Getting Started with Projects"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This section provides hands-on projects that integrate the concepts learned throughout the textbook. Each project builds on multiple topics including ROS 2, Vision-Language Models, simulation environments, and humanoid robotics control. These projects are designed to be challenging yet achievable, allowing students to apply theoretical knowledge in practical implementations."}),"\n",(0,s.jsx)(n.h2,{id:"project-structure",children:"Project Structure"}),"\n",(0,s.jsx)(n.p,{children:"Each project follows a consistent structure:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning Objectives"}),": Clear goals for what you'll learn"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),": Knowledge and tools required"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implementation Steps"}),": Detailed instructions for implementation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing"}),": How to verify your implementation works"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extensions"}),": Opportunities to extend the project further"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-1-autonomous-object-retrieval-system",children:"Project 1: Autonomous Object Retrieval System"}),"\n",(0,s.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate perception and manipulation for a complete task"}),"\n",(0,s.jsx)(n.li,{children:"Use VLMs to identify and locate objects"}),"\n",(0,s.jsx)(n.li,{children:"Implement a complete task pipeline from perception to action"}),"\n",(0,s.jsx)(n.li,{children:"Combine navigation, manipulation, and planning"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 basics"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of robot kinematics"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with perception concepts"}),"\n",(0,s.jsx)(n.li,{children:"Basic control systems knowledge"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h4,{id:"step-1-environment-setup",children:"Step 1: Environment Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create a workspace for the project\nmkdir -p ~/object_retrieval_ws/src\ncd ~/object_retrieval_ws\n\n# Build the workspace with necessary packages\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,s.jsx)(n.h4,{id:"step-2-perception-system",children:"Step 2: Perception System"}),"\n",(0,s.jsx)(n.p,{children:"The perception system uses Vision-Language Models to identify objects:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rospy\nimport numpy as np\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport torch\nimport clip  # Using CLIP for vision-language model\n\nclass ObjectRetrievalPerceptor:\n    def __init__(self):\n        rospy.init_node(\'object_retrieval_perceptor\')\n        \n        # Initialize CLIP model\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)\n        \n        # ROS interface\n        self.bridge = CvBridge()\n        self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)\n        self.object_pub = rospy.Publisher("/detected_object", Point, queue_size=10)\n        \n        # Define object vocabulary for the task\n        self.object_vocabulary = [\n            "red cup", "blue bottle", "green box", \n            "yellow book", "black bag", "white mug"\n        ]\n        \n        self.current_image = None\n    \n    def image_callback(self, msg):\n        # Convert ROS image to PIL Image\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n        self.current_image = cv_image\n        \n    def identify_object(self, target_descriptor):\n        """\n        Identify and locate an object based on a text descriptor\n        """\n        if self.current_image is None:\n            return None\n        \n        # Preprocess image\n        image_input = self.preprocess(self.current_image).unsqueeze(0).to(self.device)\n        \n        # Create text descriptions\n        text_descriptions = [f"a photo of {obj}" for obj in self.object_vocabulary]\n        text_inputs = torch.cat([clip.tokenize(desc) for desc in text_descriptions]).to(self.device)\n        \n        with torch.no_grad():\n            # Get image and text features\n            image_features = self.model.encode_image(image_input)\n            text_features = self.model.encode_text(text_inputs)\n            \n            # Calculate similarities\n            similarities = (image_features @ text_features.T).softmax(dim=-1)\n            \n            # Find the best match\n            best_match_idx = similarities[0].argmax().item()\n            best_match_score = similarities[0][best_match_idx].item()\n            best_match_object = self.object_vocabulary[best_match_idx]\n            \n            # Only return if it matches the target descriptor and confidence is high\n            if target_descriptor.lower() in best_match_object.lower() and best_match_score > 0.7:\n                # In a real implementation, you would need to extract the position\n                # For this example, we\'ll return a dummy position\n                object_position = Point(x=1.0, y=0.5, z=0.0)\n                return object_position, best_match_object, best_match_score\n        \n        return None, None, 0.0\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-3-navigation-system",children:"Step 3: Navigation System"}),"\n",(0,s.jsx)(n.p,{children:"Implement navigation to approach the object location:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rospy\nfrom geometry_msgs.msg import Twist, Point\nfrom nav_msgs.msg import Odometry\nfrom tf.transformations import euler_from_quaternion\nimport actionlib\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\n\nclass NavigationController:\n    def __init__(self):\n        rospy.init_node(\'navigation_controller\')\n        \n        # Action client for move_base\n        self.move_base_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\n        rospy.loginfo("Waiting for move_base action server...")\n        self.move_base_client.wait_for_server()\n        rospy.loginfo("Connected to move_base action server")\n        \n        # Robot pose\n        self.current_pose = None\n        self.odom_sub = rospy.Subscriber(\'/odom\', Odometry, self.odom_callback)\n    \n    def odom_callback(self, msg):\n        self.current_pose = msg.pose.pose\n    \n    def navigate_to(self, target_position):\n        """\n        Navigate the robot to a target position\n        """\n        goal = MoveBaseGoal()\n        goal.target_pose.header.frame_id = "map"\n        goal.target_pose.header.stamp = rospy.Time.now()\n        \n        # Set the target position\n        goal.target_pose.pose.position = target_position\n        # Set a default orientation (facing forward)\n        goal.target_pose.pose.orientation.w = 1.0\n        \n        rospy.loginfo(f"Navigating to position: ({target_position.x}, {target_position.y})")\n        \n        # Send goal to move_base\n        self.move_base_client.send_goal(goal)\n        \n        # Wait for result\n        finished_within_time = self.move_base_client.wait_for_result(rospy.Duration(300))  # 5 minutes timeout\n        \n        if not finished_within_time:\n            self.move_base_client.cancel_goal()\n            rospy.logerr("Navigation timed out")\n            return False\n        \n        state = self.move_base_client.get_state()\n        if state == 3:  # GoalState.SUCCEEDED\n            rospy.loginfo("Navigation succeeded")\n            return True\n        else:\n            rospy.logerr(f"Navigation failed with state: {state}")\n            return False\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-4-manipulation-system",children:"Step 4: Manipulation System"}),"\n",(0,s.jsx)(n.p,{children:"Implement the arm control to grasp the object:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rospy\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport actionlib\nfrom control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\n\nclass ManipulationController:\n    def __init__(self):\n        rospy.init_node(\'manipulation_controller\')\n        \n        # Joint trajectory publisher for arm\n        self.arm_traj_pub = rospy.Publisher(\'/arm_controller/command\', JointTrajectory, queue_size=10)\n        \n        # Gripper control\n        self.gripper_pub = rospy.Publisher(\'/gripper_controller/command\', String, queue_size=10)\n        \n        # Action client for more complex trajectories\n        self.arm_client = actionlib.SimpleActionClient(\'/arm_controller/follow_joint_trajectory\', \n                                                      FollowJointTrajectoryAction)\n    \n    def grasp_object(self, object_position):\n        """\n        Execute a grasping motion to pick up an object\n        """\n        # Approach the object\n        approach_poses = self.calculate_approach_poses(object_position)\n        \n        # Execute approach trajectory\n        for pose in approach_poses:\n            success = self.move_arm_to_pose(pose)\n            if not success:\n                rospy.logerr("Failed to move arm to approach position")\n                return False\n        \n        # Close gripper to grasp\n        self.close_gripper()\n        \n        # Lift the object\n        lift_trajectory = self.calculate_lift_trajectory()\n        success = self.execute_trajectory(lift_trajectory)\n        \n        if success:\n            rospy.loginfo("Successfully grasped the object!")\n            return True\n        else:\n            rospy.logerr("Failed to grasp the object")\n            return False\n    \n    def calculate_approach_poses(self, object_position):\n        """\n        Calculate a trajectory of poses to approach the object\n        """\n        # This would contain inverse kinematics calculations\n        # For this example, we\'ll return a simple trajectory\n        approach_poses = [\n            # Pre-grasp position (slightly above object)\n            [object_position.x, object_position.y, object_position.z + 0.1, 0, 0, 0],\n            # Grasp position (at object height)\n            [object_position.x, object_position.y, object_position.z, 0, 0, 0]\n        ]\n        return approach_poses\n    \n    def move_arm_to_pose(self, pose):\n        """\n        Move the arm to a specific pose\n        """\n        # Implementation would use inverse kinematics\n        # and publish trajectory commands\n        trajectory = self.create_trajectory_for_pose(pose)\n        return self.execute_trajectory(trajectory)\n    \n    def create_trajectory_for_pose(self, pose):\n        """\n        Create a joint trajectory to reach a specific pose\n        """\n        trajectory = JointTrajectory()\n        trajectory.joint_names = ["shoulder_joint", "elbow_joint", "wrist_joint"]  # Example\n        \n        point = JointTrajectoryPoint()\n        # Calculate joint angles for pose using inverse kinematics\n        joint_angles = self.inverse_kinematics(pose)  # This would be implemented\n        point.positions = joint_angles\n        point.time_from_start = rospy.Duration(2.0)  # 2 seconds to reach pose\n        \n        trajectory.points.append(point)\n        return trajectory\n    \n    def execute_trajectory(self, trajectory):\n        """\n        Execute a joint trajectory\n        """\n        self.arm_traj_pub.publish(trajectory)\n        rospy.sleep(trajectory.points[-1].time_from_start)  # Wait for completion\n        return True  # Simplified success check\n    \n    def close_gripper(self):\n        """\n        Close the gripper to grasp the object\n        """\n        command = String()\n        command.data = "close"\n        self.gripper_pub.publish(command)\n        rospy.sleep(1.0)  # Wait for gripper to close\n    \n    def calculate_lift_trajectory(self):\n        """\n        Calculate a trajectory to lift the object\n        """\n        # Move up by 10cm\n        trajectory = JointTrajectory()\n        trajectory.joint_names = ["shoulder_joint", "elbow_joint", "wrist_joint"]\n        \n        # Lift trajectory point\n        point = JointTrajectoryPoint()\n        # This would be calculated based on current joint angles + lift motion\n        point.positions = [0.1, -0.5, 0.2]  # Example values\n        point.time_from_start = rospy.Duration(3.0)\n        \n        trajectory.points.append(point)\n        return trajectory\n    \n    def inverse_kinematics(self, pose):\n        """\n        Calculate joint angles to reach a specific pose (simplified)\n        """\n        # In reality, this would be a complex inverse kinematics solver\n        # For this example, return dummy values\n        return [0.0, 0.0, 0.0]  # Placeholder\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-5-system-integration",children:"Step 5: System Integration"}),"\n",(0,s.jsx)(n.p,{children:"Create the main project that ties all components together:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\n\nimport rospy\nfrom object_retrieval_perceptor import ObjectRetrievalPerceptor\nfrom navigation_controller import NavigationController\nfrom manipulation_controller import ManipulationController\nfrom geometry_msgs.msg import Point\nimport time\n\nclass ObjectRetrievalSystem:\n    def __init__(self):\n        rospy.init_node(\'object_retrieval_system\')\n        \n        # Initialize all components\n        self.perceptor = ObjectRetrievalPerceptor()\n        self.navigation = NavigationController()\n        self.manipulation = ManipulationController()\n        \n        # Main control loop rate\n        self.rate = rospy.Rate(1)  # 1 Hz\n    \n    def execute_task(self, object_description):\n        """\n        Execute the complete object retrieval task:\n        1. Identify the object position\n        2. Navigate to the object location\n        3. Manipulate and retrieve the object\n        """\n        rospy.loginfo(f"Starting object retrieval for: {object_description}")\n        \n        # Step 1: Identify object\n        rospy.loginfo("Looking for object...")\n        for attempt in range(10):  # Try for 10 times\n            object_pos, detected_obj, confidence = self.perceptor.identify_object(object_description)\n            if object_pos is not None:\n                rospy.loginfo(f"Found {detected_obj} with confidence {confidence:.2f} at {object_pos}")\n                break\n            rospy.sleep(1.0)\n        else:\n            rospy.logerr(f"Could not find {object_description}")\n            return False\n        \n        # Step 2: Navigate to object\n        rospy.loginfo("Navigating to object...")\n        # Adjust target position to approach from front, not collide with object\n        approach_position = Point(\n            x=object_pos.x - 0.5,  # 0.5m in front of object\n            y=object_pos.y,\n            z=object_pos.z\n        )\n        \n        nav_success = self.navigation.navigate_to(approach_position)\n        if not nav_success:\n            rospy.logerr("Navigation to object failed")\n            return False\n        \n        # Step 3: Manipulate and retrieve\n        rospy.loginfo("Attempting to retrieve object...")\n        grasp_success = self.manipulation.grasp_object(object_pos)\n        if not grasp_success:\n            rospy.logerr("Failed to grasp the object")\n            return False\n        \n        rospy.loginfo("Object retrieval completed successfully!")\n        return True\n    \n    def run(self):\n        """\n        Run the object retrieval system\n        """\n        # For this example, we\'ll retrieve a "red cup"\n        target_object = "red cup"\n        \n        success = self.execute_task(target_object)\n        \n        if success:\n            rospy.loginfo("Task completed successfully!")\n        else:\n            rospy.logerr("Task failed!")\n        \n        # Keep the node running\n        rospy.spin()\n\nif __name__ == \'__main__\':\n    try:\n        system = ObjectRetrievalSystem()\n        system.run()\n    except rospy.ROSInterruptException:\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"testing-the-project",children:"Testing the Project"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run the simulation environment"})," with objects to retrieve"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Launch the perception system"}),": ",(0,s.jsx)(n.code,{children:"rosrun object_retrieval object_perceptor.py"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Launch the navigation system"}),": ",(0,s.jsx)(n.code,{children:"rosrun object_retrieval navigation_controller.py"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Launch the manipulation system"}),": ",(0,s.jsx)(n.code,{children:"rosrun object_retrieval manipulation_controller.py"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run the integrated system"}),": ",(0,s.jsx)(n.code,{children:"rosrun object_retrieval object_retrieval_system.py"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"extensions",children:"Extensions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Add multiple object retrieval capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Implement obstacle avoidance during navigation"}),"\n",(0,s.jsx)(n.li,{children:"Add speech interface for object identification"}),"\n",(0,s.jsx)(n.li,{children:"Implement a placement station to deliver objects to"}),"\n",(0,s.jsx)(n.li,{children:"Add learning capabilities to improve performance over time"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-2-humanoid-robot-dance-choreography",children:"Project 2: Humanoid Robot Dance Choreography"}),"\n",(0,s.jsx)(n.h3,{id:"learning-objectives-1",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use whole-body control for expressive motion"}),"\n",(0,s.jsx)(n.li,{children:"Sequence complex movements for choreography"}),"\n",(0,s.jsx)(n.li,{children:"Implement smooth transitions between poses"}),"\n",(0,s.jsx)(n.li,{children:"Synchronize movements with audio"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites-1",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of humanoid robot kinematics"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of control systems"}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of audio processing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps-1",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h4,{id:"step-1-dance-motion-primitives",children:"Step 1: Dance Motion Primitives"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass DanceMotionPrimitives:\n    def __init__(self):\n        # Define basic dance poses (simplified example)\n        self.poses = {\n            "ready": self.get_ready_pose(),\n            "wave": self.get_wave_pose(),\n            "turn": self.get_turn_pose(),\n            "step_left": self.get_step_pose(side="left"),\n            "step_right": self.get_step_pose(side="right"),\n            "raise_arms": self.get_raise_arms_pose(),\n            "salsa_step": self.get_salsa_step_pose()\n        }\n    \n    def get_ready_pose(self):\n        """Standard standing position"""\n        return {\n            "left_hip_yaw": 0.0, "left_hip_pitch": -0.3, "left_hip_roll": 0.0,\n            "left_knee": 0.6, "left_ankle_pitch": -0.3, "left_ankle_roll": 0.0,\n            "right_hip_yaw": 0.0, "right_hip_pitch": -0.3, "right_hip_roll": 0.0,\n            "right_knee": 0.6, "right_ankle_pitch": -0.3, "right_ankle_roll": 0.0,\n            "left_shoulder_yaw": 0.0, "left_shoulder_pitch": -0.2, "left_shoulder_roll": 0.0,\n            "left_elbow": 0.5, "right_shoulder_yaw": 0.0, "right_shoulder_pitch": -0.2,\n            "right_shoulder_roll": 0.0, "right_elbow": 0.5,\n            "head_yaw": 0.0, "head_pitch": 0.0\n        }\n    \n    def get_wave_pose(self):\n        """Position for waving gesture"""\n        base = self.get_ready_pose()\n        # Modify just the arm joints for waving\n        base["left_shoulder_pitch"] = -0.5\n        base["left_shoulder_roll"] = 0.5\n        base["left_elbow"] = 1.5\n        return base\n    \n    def get_turn_pose(self):\n        """Position to facilitate turning motion"""\n        base = self.get_ready_pose()\n        # Shift weight to right leg for turning\n        base["left_hip_pitch"] = -0.2\n        base["right_hip_pitch"] = -0.4\n        base["left_ankle_pitch"] = -0.1\n        base["right_ankle_pitch"] = -0.5\n        base["left_hip_roll"] = 0.2\n        base["right_hip_roll"] = -0.2\n        return base\n    \n    def get_step_pose(self, side="left"):\n        """Position for stepping motion"""\n        base = self.get_ready_pose()\n        if side == "left":\n            # Load weight on right leg, prepare left for step\n            base["left_hip_pitch"] = -0.1  # Less weight on stepping leg\n            base["right_hip_pitch"] = -0.5  # More weight on support leg\n            base["left_knee"] = 0.3  # Slightly bent for step\n        else:\n            base["right_hip_pitch"] = -0.1\n            base["left_hip_pitch"] = -0.5\n            base["right_knee"] = 0.3\n        return base\n    \n    def get_raise_arms_pose(self):\n        """Position with arms raised"""\n        base = self.get_ready_pose()\n        base["left_shoulder_pitch"] = 1.0\n        base["left_shoulder_roll"] = 0.2\n        base["left_elbow"] = 0.2\n        base["right_shoulder_pitch"] = 1.0\n        base["right_shoulder_roll"] = -0.2\n        base["right_elbow"] = 0.2\n        return base\n    \n    def get_salsa_step_pose(self):\n        """Salsa-specific stepping pose"""\n        base = self.get_ready_pose()\n        base["left_hip_pitch"] = -0.2\n        base["right_hip_pitch"] = -0.4\n        base["left_knee"] = 0.8  # More bent for dynamic movement\n        base["right_knee"] = 0.4\n        # Add some hip movement for salsa style\n        base["left_hip_yaw"] = 0.3\n        base["right_hip_yaw"] = -0.3\n        return base\n    \n    def create_transition_trajectory(self, start_pose, end_pose, duration, steps=20):\n        """\n        Create smooth trajectory between two poses\n        """\n        trajectory = []\n        time_step = duration / steps\n        \n        for i in range(steps + 1):\n            progress = i / steps\n            # Linear interpolation between poses\n            current_pose = {}\n            for joint in start_pose:\n                start_val = start_pose[joint]\n                end_val = end_pose[joint]\n                current_val = start_val + progress * (end_val - start_val)\n                current_pose[joint] = current_val\n            \n            trajectory.append({\n                "time": i * time_step,\n                "joint_angles": current_pose\n            })\n        \n        return trajectory\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-2-choreography-sequencer",children:"Step 2: Choreography Sequencer"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import time\nimport threading\nfrom dance_motion_primitives import DanceMotionPrimitives\n\nclass ChoreographySequencer:\n    def __init__(self):\n        self.primitives = DanceMotionPrimitives()\n        self.current_move = "ready"\n        self.is_performing = False\n        self.performance_thread = None\n        \n    def sequence_basic_dance(self):\n        """\n        Sequence a basic dance routine\n        """\n        # Define a simple sequence of moves\n        sequence = [\n            {"move": "wave", "duration": 2.0},\n            {"move": "raise_arms", "duration": 1.5},\n            {"move": "step_left", "duration": 1.0},\n            {"move": "step_right", "duration": 1.0},\n            {"move": "salsa_step", "duration": 1.0},\n            {"move": "turn", "duration": 2.0},\n            {"move": "ready", "duration": 1.0}\n        ]\n        \n        # Repeat the sequence 3 times\n        full_sequence = sequence * 3\n        \n        return full_sequence\n    \n    def execute_choreography(self, sequence, tempo=120):\n        """\n        Execute a choreographed sequence of movements\n        """\n        if self.is_performing:\n            rospy.logwarn("Already performing a sequence, skipping request")\n            return False\n        \n        self.is_performing = True\n        rospy.loginfo("Starting dance performance")\n        \n        current_pose = self.primitives.poses["ready"]\n        \n        for i, move_spec in enumerate(sequence):\n            if not self.is_performing:\n                rospy.loginfo("Performance stopped by user")\n                break\n                \n            move_name = move_spec["move"]\n            duration = move_spec["duration"]\n            \n            rospy.loginfo(f"Executing move {i+1}/{len(sequence)}: {move_name}")\n            \n            # Get the target pose\n            target_pose = self.primitives.poses[move_name]\n            \n            # Create and execute transition\n            transition = self.primitives.create_transition_trajectory(\n                current_pose, target_pose, duration\n            )\n            \n            # Execute the movement\n            self.execute_trajectory(transition)\n            \n            # Update current pose\n            current_pose = target_pose\n            \n            rospy.loginfo(f"Completed move: {move_name}")\n        \n        rospy.loginfo("Dance performance completed")\n        self.is_performing = False\n        return True\n    \n    def execute_trajectory(self, trajectory):\n        """\n        Execute a trajectory by publishing to joint controllers\n        """\n        # In a real implementation, this would publish to ROS topics\n        # For simulation, we\'ll just sleep for the duration\n        total_duration = trajectory[-1]["time"] if trajectory else 0\n        time.sleep(total_duration)\n    \n    def start_performance(self):\n        """\n        Start the dance performance in a separate thread\n        """\n        sequence = self.sequence_basic_dance()\n        \n        self.performance_thread = threading.Thread(\n            target=self.execute_choreography,\n            args=(sequence,)\n        )\n        self.performance_thread.start()\n    \n    def stop_performance(self):\n        """\n        Stop the current performance\n        """\n        self.is_performing = False\n        if self.performance_thread:\n            self.performance_thread.join()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"testing-the-project-1",children:"Testing the Project"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Load the humanoid robot model in your simulation environment"}),"\n",(0,s.jsx)(n.li,{children:"Run the choreography sequencer"}),"\n",(0,s.jsx)(n.li,{children:"Start a basic dance performance"}),"\n",(0,s.jsx)(n.li,{children:"Observe the robot executing the programmed dance moves"}),"\n",(0,s.jsx)(n.li,{children:"Extend the sequence with additional moves"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"extensions-1",children:"Extensions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Add audio synchronization capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Implement learning from human demonstrations"}),"\n",(0,s.jsx)(n.li,{children:"Add interactive elements responding to music"}),"\n",(0,s.jsx)(n.li,{children:"Enable real-time performance modifications"}),"\n",(0,s.jsx)(n.li,{children:"Implement crowd-responsive behaviors"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-3-educational-tutor-robot",children:"Project 3: Educational Tutor Robot"}),"\n",(0,s.jsx)(n.h3,{id:"learning-objectives-2",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement adaptive tutoring using AI"}),"\n",(0,s.jsx)(n.li,{children:"Integrate HRI principles for teaching"}),"\n",(0,s.jsx)(n.li,{children:"Develop personalized learning paths"}),"\n",(0,s.jsx)(n.li,{children:"Use multimodal interaction (speech, gesture, visual)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites-2",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of HRI principles"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of AI and machine learning concepts"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with educational psychology basics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps-2",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h4,{id:"step-1-student-model-tracker",children:"Step 1: Student Model Tracker"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class StudentModel:\n    def __init__(self, student_id):\n        self.student_id = student_id\n        self.knowledge_state = {}  # Concepts and mastery levels\n        self.learning_style = "balanced"  # visual, auditory, kinesthetic, balanced\n        self.motivation_level = 0.7\n        self.confusion_indicators = []\n        self.progress_history = []\n    \n    def update_knowledge(self, concept, mastery_level):\n        """\n        Update the student\'s mastery level for a concept\n        """\n        self.knowledge_state[concept] = {\n            "mastery": mastery_level,  # 0.0 to 1.0\n            "last_attempt": time.time(),\n            "attempts": self.knowledge_state.get(concept, {}).get("attempts", 0) + 1\n        }\n    \n    def is_confused(self):\n        """\n        Determine if the student is currently confused\n        """\n        # Simple heuristic: if multiple recent attempts failed, consider confused\n        recent_confusion = any(c["timestamp"] > time.time() - 300 for c in self.confusion_indicators[-5:])  # Last 5 minutes\n        return recent_confusion\n    \n    def get_difficulty_recommendation(self, concept):\n        """\n        Recommend difficulty level based on student\'s mastery\n        """\n        current_mastery = self.knowledge_state.get(concept, {}).get("mastery", 0.0)\n        \n        if current_mastery < 0.3:\n            return "basic"\n        elif current_mastery < 0.7:\n            return "intermediate"\n        else:\n            return "advanced"\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-2-adaptive-tutor-controller",children:"Step 2: Adaptive Tutor Controller"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from student_model import StudentModel\nfrom text_to_speech import TextToSpeech\nfrom speech_recognition import SpeechRecognizer\n\nclass AdaptiveTutor:\n    def __init__(self, robot_name="EducationalBot"):\n        self.robot_name = robot_name\n        self.current_student = None\n        self.concept_repository = self.load_concepts()\n        self.tts = TextToSpeech()\n        self.speech_rec = SpeechRecognizer()\n        \n    def load_concepts(self):\n        """\n        Load educational content repository\n        """\n        # This would typically load from a database or file\n        return {\n            "math": {\n                "addition": {\n                    "basic": "Let\'s learn addition! Addition is when we combine numbers together.",\n                    "intermediate": "Now let\'s try adding larger numbers with carrying over.",\n                    "advanced": "Let\'s solve complex addition problems with multiple digits."\n                },\n                "subtraction": {\n                    "basic": "Subtraction is taking away one number from another.",\n                    "intermediate": "Let\'s try subtraction with borrowing.",\n                    "advanced": "Let\'s solve complex subtraction word problems."\n                }\n            },\n            "science": {\n                "gravity": {\n                    "basic": "Gravity is a force that pulls things down.",\n                    "intermediate": "Gravity is the force that keeps planets in orbit.",\n                    "advanced": "Gravity is described by Einstein\'s theory of general relativity."\n                }\n            }\n        }\n    \n    def start_tutoring_session(self, student_id):\n        """\n        Initialize a tutoring session for a student\n        """\n        self.current_student = StudentModel(student_id)\n        self.introduce_robot()\n        \n    def introduce_robot(self):\n        """\n        Introduce the tutoring robot to the student\n        """\n        introduction = f"Hello! I\'m {self.robot_name}, your educational assistant. " \\\n                      f"I\'m here to help you learn. What subject would you like to explore today?"\n        self.tts.speak(introduction)\n    \n    def conduct_lesson(self, subject, concept):\n        """\n        Conduct an adaptive lesson based on student model\n        """\n        if not self.current_student:\n            self.tts.speak("Please start a session first.")\n            return\n        \n        # Determine appropriate difficulty\n        difficulty = self.current_student.get_difficulty_recommendation(concept)\n        \n        # Get appropriate content\n        content = self.concept_repository[subject][concept].get(difficulty, \n                                                              self.concept_repository[subject][concept]["basic"])\n        \n        # Deliver content with appropriate modalities based on learning style\n        self.deliver_content(content, difficulty)\n        \n        # Assess understanding\n        self.assess_understanding(concept)\n    \n    def deliver_content(self, content, difficulty):\n        """\n        Deliver content using appropriate modalities\n        """\n        # Speak the content\n        self.tts.speak(content)\n        \n        # Show visual aids if available\n        self.display_visual_content(content)\n        \n        # Use gestures appropriate to content\n        self.use_educational_gestures(content)\n    \n    def assess_understanding(self, concept):\n        """\n        Assess if the student understood the concept\n        """\n        # Ask a question about the concept\n        question = self.generate_question(concept)\n        \n        self.tts.speak(question)\n        \n        # Listen for student response\n        response = self.speech_rec.listen_for_response(timeout=30)\n        \n        # Evaluate response\n        is_correct = self.evaluate_response(response, concept)\n        \n        # Update student model\n        mastery_change = 0.1 if is_correct else -0.1\n        current_mastery = self.current_student.knowledge_state.get(concept, {}).get("mastery", 0.0)\n        new_mastery = max(0.0, min(1.0, current_mastery + mastery_change))\n        \n        self.current_student.update_knowledge(concept, new_mastery)\n        \n        # Provide feedback\n        if is_correct:\n            self.tts.speak("Great job! You understand this concept well.")\n            self.use_positive_gesture()\n        else:\n            self.tts.speak("That\'s okay, let me explain this concept again in a different way.")\n            self.use_encouraging_gesture()\n    \n    def generate_question(self, concept):\n        """\n        Generate an appropriate question for the concept and student level\n        """\n        # Simplified question generation\n        basic_questions = {\n            "addition": "What is 2 plus 2?",\n            "subtraction": "What is 5 minus 3?",\n            "gravity": "What force pulls things down?"\n        }\n        \n        return basic_questions.get(concept, f"Can you explain {concept} to me?")\n    \n    def evaluate_response(self, response, concept):\n        """\n        Evaluate if the response is correct\n        """\n        # Simplified evaluation\n        correct_responses = {\n            "addition": ["4", "four"],\n            "subtraction": ["2", "two"],\n            "gravity": ["gravity", "down", "force"]\n        }\n        \n        if concept in correct_responses:\n            response_lower = response.lower() if response else ""\n            return any(correct in response_lower for correct in correct_responses[concept])\n        \n        # For open-ended questions, we can\'t easily evaluate\n        return True  # Assume positive for now\n    \n    def display_visual_content(self, content):\n        """\n        Display visual content on robot\'s screen or in environment\n        """\n        # This would interface with the robot\'s display system\n        # For simulation, we\'ll just print\n        print(f"Displaying visual content: {content[:50]}...")\n    \n    def use_educational_gestures(self, content):\n        """\n        Use appropriate gestures to enhance learning\n        """\n        # Use gestures based on content keywords\n        if "addition" in content.lower() or "plus" in content.lower():\n            # Use gesture for combining/adding\n            pass\n        elif "subtraction" in content.lower() or "minus" in content.lower():\n            # Use gesture for taking away\n            pass\n    \n    def use_positive_gesture(self):\n        """\n        Use a positive gesture (like nodding or thumbs up)\n        """\n        print("Robot performs positive gesture")\n    \n    def use_encouraging_gesture(self):\n        """\n        Use an encouraging gesture\n        """\n        print("Robot performs encouraging gesture")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-3-main-tutoring-system",children:"Step 3: Main Tutoring System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\n\nimport rospy\nfrom adaptive_tutor import AdaptiveTutor\n\nclass EducationalTutorSystem:\n    def __init__(self):\n        rospy.init_node(\'educational_tutor_system\')\n        \n        self.tutor = AdaptiveTutor("LearningBot")\n        self.session_active = False\n        \n        # Create service for starting sessions\n        from std_srvs.srv import Trigger, TriggerResponse\n        self.session_service = rospy.Service(\'start_tutoring\', Trigger, self.handle_start_session)\n        \n        rospy.loginfo("Educational Tutor System initialized")\n    \n    def handle_start_session(self, req):\n        """\n        Handle request to start a tutoring session\n        """\n        if not self.session_active:\n            self.tutor.start_tutoring_session("student123")\n            self.session_active = True\n            rospy.loginfo("Tutoring session started")\n            return TriggerResponse(success=True, message="Session started")\n        else:\n            return TriggerResponse(success=False, message="Session already active")\n    \n    def run(self):\n        """\n        Run the educational tutoring system\n        """\n        rospy.loginfo("Educational Tutor System running - waiting for requests")\n        rospy.spin()\n\nif __name__ == \'__main__\':\n    try:\n        system = EducationalTutorSystem()\n        system.run()\n    except rospy.ROSInterruptException:\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"testing-the-project-2",children:"Testing the Project"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Deploy the tutoring system on your educational platform"}),"\n",(0,s.jsx)(n.li,{children:"Start a tutoring session via the service"}),"\n",(0,s.jsx)(n.li,{children:"Interact with the robot tutor through speech/interaction"}),"\n",(0,s.jsx)(n.li,{children:"Observe how the system adapts to your responses"}),"\n",(0,s.jsx)(n.li,{children:"Check how the student model updates based on interactions"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"extensions-2",children:"Extensions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Add multiple subject areas with detailed content"}),"\n",(0,s.jsx)(n.li,{children:"Implement more sophisticated student modeling"}),"\n",(0,s.jsx)(n.li,{children:"Add progress tracking and reporting"}),"\n",(0,s.jsx)(n.li,{children:"Include collaborative learning features"}),"\n",(0,s.jsx)(n.li,{children:"Add gamification elements to increase engagement"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"These projects demonstrate the integration of multiple concepts from the textbook. Each project combines perception, control, interaction, and AI to create meaningful applications. The projects can be extended and customized based on specific educational or research goals."}),"\n",(0,s.jsx)(n.p,{children:"The key to success in these projects is systematic testing, iterative development, and consideration of human factors in robot design and interaction."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(6540);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);